# -*- coding: utf-8 -*-
"""Experimental Design - Blocking Factor & Continuous Covariate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbQVeuOXE9jbBmTBavBffdH5_8Cymd0b

#**Designing 5 Types of Randomized Control Trials (RCTs): Analyzing the Impact of Price Markdown Strategies on E-commerce**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""#**Synthetic Data** <br>
Platform: Shopper's Drug Mart <br>
Number of Participants: 10000<br>
Demographic info: Age, Gender, Location, Income <br>
User Purchase Behavior: Past Purchase History, Frequency of Visits, Frequency of Made Purchase, Types of Purchases <br>
Dependent Variables: Session Duration, Conversion Rate, Number of Items, Average Order Value (Add two more variables Average Item Value * Number of Items Purchased to compute the AOV)

Note: We generated the independent, demographic variables below. We then chose to generate the dependent variables such as Frequency of Visits,	Frequency of Made Purchase, Conversion Rate, and Session Duration within each experiment separately.

"""

# Define constants
# Total participants
n_participants = 10000
# Total visits last year
historical_visits = np.random.randint(0, 100, n_participants)
# Threshold for being considered a frequent user
frequent_threshold = 50

# Determine frequency in past based on threshold
frequency_in_past = ['frequent' if visits > frequent_threshold else 'not frequent' for visits in historical_visits]

# Generate data
participants_data = {
    "Participant ID": np.arange(1, n_participants + 1),
    "Age": np.random.randint(18, 71, n_participants),
    "Gender": np.random.choice(['Male', 'Female'], n_participants),
    "Location": np.random.choice(['Toronto', 'Montreal', 'Vancouver', 'Calgary', 'Edmonton',
                                  'Ottawa', 'Winnipeg', 'Quebec City', 'Hamilton', 'Kitchener',
                                  'London', 'Victoria', 'Halifax', 'Oshawa', 'Windsor',
                                  'Saskatoon', 'Regina', 'St. John\'s', 'Kelowna', 'Barrie'], n_participants),
    "Income": np.random.randint(0, 200001, n_participants),
    "Frequency in the Past": frequency_in_past,
    "Historical Visits": np.random.randint(0, 200, n_participants),
    "Device Type": np.random.choice(["Mobile", "Desktop", "Tablet"], n_participants),
    "Preferred Payment Method": np.random.choice(["Credit Card", "PayPal", "Bank Transfers", "Buy Now Pay Later", "Digital Wallet", "Debit"], n_participants),
    "Membership Status": np.random.choice(["Basic", "Silver", "Gold", "Platinum"], n_participants),
    "Exposure to Social Media Promotion": np.random.choice(["Yes", "No"], n_participants)
}

# Create DataFrame from participants data
df = pd.DataFrame(participants_data)
df.head(15)

"""#**1. Parallel Design**
â€‹
**Section 1.1: Power Analysis, Sample Size, & Associated Graph** </br>
>Effect Size: Medium (0.5)</br>
Power: 0.85 <br>
Total Sample Size: 146 </br>



"""

from statsmodels.stats.power import TTestIndPower

# Power Analysis for Parallel Design Scenario

# Medium effect size
effect_size = 0.5
# Significance level
alpha = 0.05
# Power value
power = 0.85
# parallel design
number_groups = 2

# Creating a power analysis object
analysis = TTestIndPower()

# Calculating the required sample size per group
result = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power)

# Rounding to get integer sample size
required_sample_size_per_group = round(result)

# Total sample size required for the experiment
total_sample_size = required_sample_size_per_group * number_groups

# Output the result
print(f'Required sample size per group: {required_sample_size_per_group}')
print(f'Total sample size for all groups: {total_sample_size}')

from numpy import array
from matplotlib import pyplot

# Effect sizes: small (0.2), medium (0.5), large (0.8)
effect_sizes = array([0.2, 0.5, 0.8])

# Sample sizes: range from 2 to 146
sample_sizes = array(range(2, 146))

# Create a power analysis object for t-tests
analysis = TTestIndPower()

# Plot power curves
analysis.plot_power(dep_var='nobs', nobs=sample_sizes, effect_size=effect_sizes, alpha=0.05, title='Power of t-Test')

# Formatting the plot
pyplot.grid()
pyplot.ylabel('Power')
pyplot.show()

"""**Section 1.2: BLOCKING, TWO-WAY ANOVA, POST-HOC TUKEY'S HSD TEST & ASSUMPTION CHECKS FOR ANOVA** <br>

><mark>**Assignment 2 - Blocking for parallel analysis using Membership Status as blocking factor** <br>

> Platinum level customers might be more willing to purchase, regardless of the promotion. <br>
Confounding Effect: If a specific promotion is used more often in platinum level vs. another one in the gold, silver, basic levels, the promotion's effect on conversion may be overestimated. <br>
"""

# Setting the seed for reproducibility: so that our analysis and conclusions remain consistent
np.random.seed(42)

# Define membership levels
membership_labels = ["Basic", "Silver", "Gold", "Platinum"]

# Assign random membership status to each participant
df["Membership_Status"] = np.random.choice(membership_labels, size=len(df), replace=True)

# Create a dictionary to store the participants by membership status
membership_samples = {}

# Loop through each membership level and randomly select 36 participants
for membership_level in membership_labels:
    selected = df[df["Membership_Status"] == membership_level].sample(n=36, random_state=42)
    membership_samples[membership_level] = selected

# Combine all selected participants into one DataFrame
selected_participants = pd.concat(membership_samples.values())

# Find remaining participants who were not selected
remaining_participants = df[~df["Participant ID"].isin(selected_participants["Participant ID"])]

# Randomly select 2 extra participants from the remaining ones
extra_participant = remaining_participants.sample(n=2, random_state=42)

# Randomly assign a membership status to the extra participants
extra_participant["Membership_Status"] = np.random.choice(membership_labels, size=2, replace=True)

# Add the extra participants to the selected participants
selected_participants = pd.concat([selected_participants, extra_participant])

# Reset index while keeping the original Participant ID
selected_participants.reset_index(drop=True, inplace=True)

# Count how many participants are in each membership status
membership_counts = selected_participants["Membership_Status"].value_counts()

# Print out the number of participants in each membership status & display the dataframe
print(membership_counts)
print("\n")

selected_features = ["Participant ID", 'Income', "Membership_Status"]
# Displaying the specific subset
selected_participants[selected_features].head(10)

""">**Randomization for Parallel Design After Blocking Done Above**"""

# Setting the seed for reproducibility
np.random.seed(42)

# Now we must randomly assign 73 participants to each of the 2 groups:
# Group 1: Buy One Get One Free
# Group 2: No Discount

# Defining the group combinations based on above parallel design
parallel_conditions = [
    ('Buy One Get One Free'),  # Group 1
    ('No Discount')  # Group 2
]

# There will be 2 groups
num_groups_p = len(parallel_conditions)
# Based on the power analysis, we will evenly split 73 individuals per group
group_size_p = len(selected_participants) // num_groups_p

# Initializing the list to store assigned treatment levels
treatment_assignments = []

# Randomly assign participants to groups by shuffling the IDs
np.random.shuffle(selected_participants['Participant ID'].values)

# Assigning participants evenly to parallel design groups using a loop
for i in range(len(selected_participants)):
    # Cycling through the 2 groups in the given order
    group_index = i % num_groups_p
    # Getting the corresponding treatment levels
    treatment = parallel_conditions[group_index]
    # Appending the assigned discount condition
    treatment_assignments.append(treatment)

# Adding the factor assignments to the DataFrame
selected_participants['DiscountType'] = treatment_assignments

# Now we will generate the dependent variables for this experiment
n_samples = len(selected_participants)

# Initializing activity data with zeros as placeholders
frequency_of_visits = np.zeros(n_samples)
frequency_of_made_purchase = np.zeros(n_samples)

# Define mean and standard deviation values for visit frequency based on treatment type
# Higher expected visits for promotional offer
visit_mean_treatment, visit_std_treatment = 30, 10
# Lower expected visits without discount
visit_mean_control, visit_std_control = 12, 5
# Standard deviation as a fraction of mean visits to account for variability
purchase_std_factor = 0.5

# Looping through each of the rows
for i in range(n_samples):
    # Holds the treatment type ('Buy One Get One Free' or 'No Discount') for the current participant
    treatments = treatment_assignments[i]

    if treatments == 'Buy One Get One Free':
        # Generate frequency of visits from normal distribution, with non-negative values
        frequency_of_visits[i] = max(0, int(np.random.normal(visit_mean_treatment, visit_std_treatment)))
    else:
        # Generate frequency of visits from normal distribution, with non-negative values
        frequency_of_visits[i] = max(0, int(np.random.normal(visit_mean_control, visit_std_control)))

    # Generate purchase frequency based on visits, ensuring it does not exceed visits
    # Assuming 50% conversion rate on average
    purchase_mean = frequency_of_visits[i] * 0.5
    # Define variability in purchases
    purchase_std = purchase_mean * purchase_std_factor
    # Generate the number of purchases, ensuring it is within valid bounds (0 to total visits)
    frequency_of_made_purchase[i] = max(0, min(frequency_of_visits[i], int(np.random.normal(purchase_mean, purchase_std))))

# Calculating conversion rate: purchases made to visits but accounting for zero values
conversion_rate = np.divide(frequency_of_made_purchase, frequency_of_visits, where=frequency_of_visits > 0)
# Replacing any strange values with 0 (i.e. NaN, negative & positive infinity)
conversion_rate = np.nan_to_num(conversion_rate, nan=0, posinf=0, neginf=0)
conversion_rate = np.round(conversion_rate, 5)

# Rounding the values for the dependent variables
frequency_of_visits = np.round(frequency_of_visits, 2)
frequency_of_made_purchase = np.round(frequency_of_made_purchase, 2)

# Creating activity DataFrame
activity_data = pd.DataFrame({
    "Participant ID": selected_participants["Participant ID"],
    "Frequency of Visits": frequency_of_visits,
    "Frequency of Made Purchase": frequency_of_made_purchase,
    "Conversion_Rate": conversion_rate
})

# Merging the membership status and activity data together
df_parallel = selected_participants.merge(activity_data, on="Participant ID")

# Count the number of participants in each treatment group
treatment_counts = selected_participants['DiscountType'].value_counts()

# Print out the counts to make sure they are evenly distributed
print(treatment_counts)
print("\n")

# Selecting the subset features that should be displayed for parallel design
subset_features = [
    'Participant ID', 'Income', 'Membership_Status', 'DiscountType',
    'Frequency of Visits', 'Frequency of Made Purchase', 'Conversion_Rate'
]

# Show the demographic data for the selected participants
df_parallel[subset_features].head(10)

""">**Checking for Normality of Dependent Variable: Conversion Rate**"""

# Create a histogram of the Conversion Rate
plt.figure(figsize=(10, 6))
plt.hist(df_parallel['Conversion_Rate'].dropna(), bins=30, color='blue', alpha=0.7)
# Adds a title to the histogram
plt.title('Distribution of Conversion Rate (i.e. Ratios)')
# Label for the x-axis
plt.xlabel('Conversion Rate')
# Label for the y-axis
plt.ylabel('Frequency')
# Adds a grid for better readability
plt.grid(True)
# Displays the plot
plt.show()

""">**Violin Plot to Display the Distribution after the Blocking & Randomization for Parallel Design**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create the violin plot
plt.figure(figsize=(12, 6))
sns.violinplot(x="DiscountType", y="Conversion_Rate", data=df_parallel, hue='DiscountType',
               order=["Buy One Get One Free", "No Discount"])

# Add title and labels
plt.title('Violin Plot of Conversion Rate by Discount Type Groups After Membership Level Blocking', fontsize=16)
plt.xlabel('Discount Type', fontsize=12)
plt.ylabel('Conversion Rate', fontsize=12)

# Show the plot
plt.show()

""">**Boxplot: A more clear visualization of the distribution after separating into different Membership levels and discount types (i.e. treatment/control groups)**"""

import seaborn as sns
import matplotlib.pyplot as plt

# Filter the data (assuming you want to filter for relevant levels of membership status and discount type)
df_parallel_cleaned = df_parallel.dropna(subset=['Conversion_Rate', 'DiscountType', 'Membership_Status'])

# Plot the boxplot with 'DiscountType' on x-axis, 'Conversion_Rate' on y-axis, and 'Membership_Status' as hue
plt.figure(figsize=(14, 6))
ax = sns.boxplot(data=df_parallel_cleaned, x="DiscountType", y="Conversion_Rate", hue="Membership_Status")

ax.set_xticks([0, 1])
ax.set_xticklabels(['No Discount', 'BOGOF'], rotation=0)

# Customize the plot (labels, title, etc.)
ax.set_title('Conversion Rate by Discount Type and Membership Status')
ax.set_xlabel('Discount Type')
ax.set_ylabel('Conversion Rate')

# Show the plot and the legend for membership status
plt.legend(title='Membership Status', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

""">**Shapiro-Wilk test: We use the Shapiro-Wilks Test to check if the set of observations comes from a normal distribution before fitting the two-way ANOVA.**"""

from scipy import stats

# Perform Shapiro-Wilk test on Conversion_Rate and print result
shapiroUnem = stats.shapiro(df_parallel['Conversion_Rate'].dropna())

print(f'W: {round(shapiroUnem[0],3)}, P-value: {round(shapiroUnem[1],3)}')

""">**TWO-WAY ANOVA & SUMMARY STATISTICS: After blocking using membership status level and then randomly assigning each participant into two groups we use two-way ANOVA to find the significance of the blocking factor based on the p-value**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Create model
formula = 'Conversion_Rate ~ DiscountType + Membership_Status + DiscountType:Membership_Status'
model = sm.formula.ols(formula, data=df_parallel).fit()

# Perform ANOVA and print table
aov_table = sm.stats.anova_lm(model)
print(aov_table)
print(model.summary())

"""**TUKEY'S HSD TEST: Post-hoc analysis following Two-Way ANOVA**

> SOURCE: https://campus.datacamp.com/courses/experimental-design-in-python/analyzing-experimental-data-statistical-tests-and-power?ex=5
"""

from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Perform Tukey's HSD test for DiscountType
tukey_results = pairwise_tukeyhsd(df_parallel['Conversion_Rate'], df_parallel['DiscountType'])

print("\nTukey HSD Test Results for DiscountType:")
print(tukey_results)

"""**Checking Assumptions for Two-Way ANOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using


"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residuals = model.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residuals, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residuals, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

""">**Performing Shapiro-Wilk test on residuals after fitting the model to check for assumption of normality.**

> The residuals of our model seem to follow a normal distribution, as the Shapiro-Wilk test results indicate there is no significant deviation from normality. It implies that the assumptions of normality for our residuals are likely met, which is an important condition for fitting our model.
"""

# Perform Shapiro-Wilk test on residuals
shapiro_res = stats.shapiro(residuals.dropna())

print(f'W: {round(shapiro_res[0], 3)}, P-value: {round(shapiro_res[1], 3)}')

""">**The Residuals vs. Fitted Values Plot:** below indicates that the model is a good fit for the data, as there are no clear patterns, trends, or heteroscedasticity visible in the residuals."""

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residuals = model.resid
fitted_values = model.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_values, y=residuals, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Section 1.3: CONTINUOUS COVARIATES, ANCOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Adding 1 New Continuous Covariate (i.e. Total_Time_Spent_On_Platform) and Using Age as the other continuous covariate** <br>
"""

# Setting the seed for reproducibility: so that our analysis and conclusions consistent
np.random.seed(42)

# Select 141 unique random participants
unique_participants = df['Participant ID'].unique()
selected_participants = np.random.choice(unique_participants, 141, replace=False)

# Filter the DataFrame to keep only the selected participants
parallel_df = df[df['Participant ID'].isin(selected_participants)].copy()

# Now only select the demographic features associated with each participant such as (Participant ID,	Age	Gender,	Location,	Income)
# Selecting the demographic features associated with each participant
demographic_features = ['Participant ID', 'Age', 'Gender', 'Location', 'Income']
# Filtering the DataFrame to include only the selected demographic features
# Also, making sure each participant is unique and only appears once
demographic_participant_df = parallel_df[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Now we must randomly assign 47 participants to each of the 3 groups:
# Group 1:  Percentage Markdown
# Group 2:  Buy One Get One Free
# Group 3:  No Discount
# Defining the group combinations based on above parallel design
parallel_conditions = [
    # Group 1: BOGOF
    ('Buy One Get One Free'),
    # Group 2: No Discount
    ('No Discount')]

# There will be 3 groups
num_groups_p = len(parallel_conditions)
# Based on the power analysis we will evenly split 47 individuals per group
group_size_p = len(demographic_participant_df) // num_groups_p

# Initializing the list to store assigned treatment levels
treatment_assignments = []

# Randomly assign participants to groups by shuffling the IDs randomly
np.random.shuffle(demographic_participant_df['Participant ID'].values)

# Assigning participants evenly to factorial groups using a loop
for i in range(len(demographic_participant_df)):
  # Cycling through the 2 groups in the given order
  group_index = i % num_groups_p
  # Getting the corresponding treatment levels
  treatment = parallel_conditions[group_index]
  # Appending the assigned holiday condition
  treatment_assignments.append(treatment)


# Adding the factor assignments to the DataFrame
demographic_participant_df['DiscountType'] = treatment_assignments


# Now we will generate the dependent variables for this experiment specifically
n_samples = len(demographic_participant_df)

# Initializing activity data and using zeros as placeholders
frequency_of_visits = np.zeros(n_samples)
frequency_of_made_purchase = np.zeros(n_samples)

# Define mean and standard deviation values for visit frequency based on treatment type
# Higher expected visits for promotional offer
visit_mean_treatment, visit_std_treatment = 30, 10
 # Lower expected visits without discount
visit_mean_control, visit_std_control = 12, 5
# Standard deviation as a fraction of mean visits to account for variability
purchase_std_factor = 0.5


# Looping through each of the rows
for i in range(n_samples):
  # Holds the treatment type ('Buy One Get One Free' or 'No Discount') for the current participant
  treatments = treatment_assignments[i]

  if treatments == 'Buy One Get One Free':
    # Generate frequency of visits from normal distribution, with non-negative values
    frequency_of_visits[i] = max(0, int(np.random.normal(visit_mean_treatment, visit_std_treatment)))
  else:
    # Generate frequency of visits from normal distribution, with non-negative values
    frequency_of_visits[i] = max(0, int(np.random.normal(visit_mean_control, visit_std_control)))

  # Generate purchase frequency based on visits, ensuring it does not exceed visits
  # Estimate the purchase frequency based on visit frequency
  # Assuming 50% conversion rate on average
  purchase_mean = frequency_of_visits[i] * 0.5
  # Define variability in purchases
  purchase_std = purchase_mean * purchase_std_factor
  # Generate the number of purchases, ensuring it is within valid bounds (0 to total visits)
  frequency_of_made_purchase[i] = max(0, min(frequency_of_visits[i], int(np.random.normal(purchase_mean, purchase_std))))

# Calculating conversion rate: which is a just the purchases made to visits but accounting for zero value
conversion_rate = np.divide(frequency_of_made_purchase, frequency_of_visits, where=frequency_of_visits > 0)
# Replacing any strange values with 0 (i.e. NaN, negative & positive infinity)
conversion_rate = np.nan_to_num(conversion_rate, nan=0, posinf=0, neginf=0)
conversion_rate = np.round(conversion_rate, 5)


# Rounding the values for the dependent variables
frequency_of_visits = np.round(frequency_of_visits, 2)
frequency_of_made_purchase = np.round(frequency_of_made_purchase, 2)


#--------------------------------------------------------------------- Assignment 2: Adding a New Covariate --------------------------------------------------------------------------#
# Create a random time multiplier between 5 and 30 minutes for each participant
# This represents how long they spend on the platform per visit (in minutes)
time_multiplier = np.random.uniform(5, 30, n_samples)

# Define how much time each treatment group spends on the platform
# We assume different discount types affect how long people stay on the platform
time_adjustment_factors = {
    # People in this group spend 50% more time on average
    'Buy One Get One Free': 1.5,
    # People in this group (control group) don't get extra time
    'No Discount': 1.0}

# Adjust the time multiplier based on the treatment assigned to each participant
adjusted_time_multiplier = []

# Looping through each participant and adjust their time multiplier based on their treatment group
for i in range(n_samples):
    # Get the treatment for this participant
    treatment = treatment_assignments[i]
    # Multiply the random time with the adjustment factor for their treatment group
    adjusted_time = time_multiplier[i] * time_adjustment_factors[treatment]
    # Appending these values to the empty list created above
    adjusted_time_multiplier.append(adjusted_time)

# Calculate the total time spent on the platform by each participant
# Multiply frequency of visits by the adjusted time multiplier for each visit
length_of_time_spent = frequency_of_visits * adjusted_time_multiplier

# Convert time from minutes to hours
# Since time is in minutes, divide by 60 to convert it to hours
length_of_time_spent_in_hours = length_of_time_spent / 60
#----------------------------------------------------------------------------END of Adding New Covariate----------------------------------------------------------------------------#

# Creating activity DataFrame
activity_data = pd.DataFrame({
    "Participant ID": demographic_participant_df["Participant ID"],
    "Frequency of Visits": frequency_of_visits,
    "Frequency_of_Made_Purchase": frequency_of_made_purchase,
    "Conversion_Rate": conversion_rate})

# Merging the demographic and activity data together
df_parallel_Covariance = demographic_participant_df.merge(activity_data, on="Participant ID")
# Adding the "Total Time Spent on Platform (hours)" to the DataFrame
df_parallel_Covariance['Total_Time_Spent_On_Platform'] = np.round(length_of_time_spent_in_hours, 2)
# Chosen features to display for this subset
chosen_features = ['Participant ID', 'Age', 'Income', 'DiscountType', 'Frequency of Visits', 'Frequency_of_Made_Purchase', 'Conversion_Rate', 'Total_Time_Spent_On_Platform']
# Show the demographic data for the selected participants
df_parallel_Covariance[chosen_features].head(10)

"""**ANCOVA, SUMMARY STATISTICS, & TUKEY HSD TEST:** <br>

> 'Age', 'Total_Time_Spent_On_Platform' are the two continuous covariates
DiscountType will be the categorical variable (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

# Create the formula for the ANCOVA model
formula_1 = 'Q("Conversion_Rate") ~ Q("Age") + Q("Total_Time_Spent_On_Platform") + C(Q("DiscountType")) + \
             Q("Age"):C(Q("DiscountType")) + Q("Total_Time_Spent_On_Platform"):C(Q("DiscountType")) + \
             Q("Total_Time_Spent_On_Platform"):Q("Age")'

# Fit the model using Ordinary Least Squares (OLS)
model_1 = sm.formula.ols(formula_1, data=df_parallel_Covariance).fit()

# Perform the Type III ANOVA and display the results
# Use typ=3 for Type III SS: used in ANCOVA because it tests the main effects and interactions, adjusting for the presence of other factors
aov_table = sm.stats.anova_lm(model_1, typ=3)
print(aov_table)
print(model_1.summary())

# Perform Tukey's HSD for DiscountType
tukey_results = pairwise_tukeyhsd(
    df_parallel_Covariance['Conversion_Rate'],
    df_parallel_Covariance['DiscountType']
)

print("\nTukey HSD Test Results:")
print(tukey_results)

"""**Checking Assumptions for ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using


"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_1 = model_1.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_1, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_1, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_resid = stats.shapiro(residual_1.dropna())

print(f'W: {round(shapiro_resid[0], 3)}, P-value: {round(shapiro_resid[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_1 = model_1.resid
fitted_value = model_1.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_1, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Section 1.4: COMBINING BLOCKING & CONTINUOUS COVARIATES, ANCOVA FOR BOTH, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - BONUS PART: Running another set of two-way ANOVAs with both the blocking factor and continuous covariates for parallel design subset of data.** <br>

"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING MEMBERSHIP STATUS -----------------------------------------------------------------------#
# Setting the seed for reproducibility
np.random.seed(42)

# Define membership levels
membership_labels = ["Basic", "Silver", "Gold", "Platinum"]

# Assign random membership status to each participant
df["Membership_Status"] = np.random.choice(membership_labels, size=len(df), replace=True)

# Create a dictionary to store the participants by membership status
membership_samples = {}

# Loop through each membership level and randomly select 36 participants
for membership_level in membership_labels:
    selected = df[df["Membership_Status"] == membership_level].sample(n=36, random_state=42)
    membership_samples[membership_level] = selected

# Combine all selected participants into one DataFrame
selected_participants = pd.concat(membership_samples.values())

# Find remaining participants who were not selected
remaining_participants = df[~df["Participant ID"].isin(selected_participants["Participant ID"])]

# Randomly select 2 extra participants from the remaining ones
extra_participant = remaining_participants.sample(n=2, random_state=42)

# Randomly assign a membership status to the extra participants
extra_participant["Membership_Status"] = np.random.choice(membership_labels, size=2, replace=True)

# Add the extra participants to the selected participants
selected_participants = pd.concat([selected_participants, extra_participant])

# Resetting index to 0 while keeping the participant id the same from original dataset
selected_participants.reset_index(drop=True, inplace=True)

# Count how many participants are in each membership status
membership_counts = selected_participants["Membership_Status"].value_counts()

# Print out the number of participants in each membership status & display the dataframe
print(membership_counts)
print("\n")

selected_features = ["Participant ID", "Income", "Membership_Status"]
# Displaying the specific subset of selected participants
selected_participants[selected_features].head(10)

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING MEMBERSHIP STATUS END -----------------------------------------------------------------------#

# Now we must randomly assign 73 participants to each of the 2 groups:
# Group 1: Buy One Get One Free
# Group 2: No Discount
parallel_conditions = [
    # Group 1: Buy one get one free
    ('Buy One Get One Free'),
    # Group 2: No Discount
    ('No Discount')]

# There will be 2 groups
num_groups_p = len(parallel_conditions)
# Based on the power analysis we will evenly split 73 individuals per group
group_size_p = len(selected_participants) // num_groups_p

# Initializing the list to store assigned treatment levels
treatment_assignments = []

# Randomly assign participants to groups by shuffling the IDs randomly
np.random.shuffle(selected_participants["Participant ID"].values)

# Assigning participants evenly to parallel design groups using a loop
for i in range(len(selected_participants)):
    # Cycling through the 2 groups in the given order
    group_index = i % num_groups_p
    # Getting the corresponding treatment levels
    treatment = parallel_conditions[group_index]
    # Appending the assigned treatment condition
    treatment_assignments.append(treatment)

# Adding the factor assignments to the DataFrame
selected_participants["DiscountType"] = treatment_assignments

# Now we will generate the dependent variables for this experiment specifically
n_samples = len(selected_participants)

# Initializing activity data and using zeros as placeholders
frequency_of_visits = np.zeros(n_samples)
frequency_of_made_purchase = np.zeros(n_samples)

# Define mean and standard deviation values for visit frequency based on treatment type
visit_mean_treatment, visit_std_treatment = 30, 10  # Higher expected visits for promotional offer
visit_mean_control, visit_std_control = 12, 5  # Lower expected visits without discount
purchase_std_factor = 0.5  # Standard deviation as a fraction of mean visits to account for variability

# Looping through each of the rows
for i in range(n_samples):
    treatments = treatment_assignments[i]

    if treatments == "Buy One Get One Free":
        frequency_of_visits[i] = max(0, int(np.random.normal(visit_mean_treatment, visit_std_treatment)))
    else:
        frequency_of_visits[i] = max(0, int(np.random.normal(visit_mean_control, visit_std_control)))

    purchase_mean = frequency_of_visits[i] * 0.5
    purchase_std = purchase_mean * purchase_std_factor
    frequency_of_made_purchase[i] = max(0, min(frequency_of_visits[i], int(np.random.normal(purchase_mean, purchase_std))))

# Calculating conversion rate: purchases made to visits (accounting for zero visits)
conversion_rate = np.divide(frequency_of_made_purchase, frequency_of_visits, where=frequency_of_visits > 0)
conversion_rate = np.nan_to_num(conversion_rate, nan=0, posinf=0, neginf=0)
conversion_rate = np.round(conversion_rate, 5)

# Rounding the values for the dependent variables
frequency_of_visits = np.round(frequency_of_visits, 2)
frequency_of_made_purchase = np.round(frequency_of_made_purchase, 2)

#--------------------------------------------------------------------- Assignment 2: Adding a New Covariate --------------------------------------------------------------------------#
time_multiplier = np.random.uniform(5, 30, n_samples)

# Adjust time multiplier for treatment groups
time_adjustment_factors = {
    'Buy One Get One Free': 1.5,  # Spend 50% more time on average
    'No Discount': 1.0  # Control group spends normal time
}

adjusted_time_multiplier = []

# Looping through each participant and adjusting their time multiplier based on treatment
for i in range(n_samples):
    treatment = treatment_assignments[i]
    adjusted_time = time_multiplier[i] * time_adjustment_factors[treatment]
    adjusted_time_multiplier.append(adjusted_time)

# Calculate the total time spent on the platform
length_of_time_spent = frequency_of_visits * adjusted_time_multiplier

# Convert time from minutes to hours
length_of_time_spent_in_hours = length_of_time_spent / 60

#----------------------------------------------------------------------------END of Adding New Covariate----------------------------------------------------------------------------#

# Creating the activity DataFrame
activity_data = pd.DataFrame({
    "Participant ID": selected_participants["Participant ID"],
    "Frequency of Visits": frequency_of_visits,
    "Frequency_of_Made_Purchase": frequency_of_made_purchase,
    "Conversion_Rate": conversion_rate
})

# Merging demographic and activity data together
df_parallel = selected_participants.merge(activity_data, on="Participant ID")

# Count the number of participants in each treatment group
treatment_counts = selected_participants["DiscountType"].value_counts()

# Print out the counts to ensure even distribution
print(treatment_counts)

print("\n")

# Adding the total time spent on platform (hours) to the DataFrame
df_parallel["Total_Time_Spent_On_Platform"] = np.round(length_of_time_spent_in_hours, 2)

# Displaying selected features
chosen_features = ['Participant ID', 'Age', 'Income', 'Membership_Status', 'DiscountType', 'Frequency of Visits', 'Frequency_of_Made_Purchase', 'Conversion_Rate', 'Total_Time_Spent_On_Platform']
df_parallel[chosen_features].head(10)

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Age', 'Total_Time_Spent_On_Platform' are the two continuous covariates, Income_Level will be a categorical blocking factor, and DiscountType will be the categorical variable (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

#------------------------------------------------ TWO-WAY ANOVA FOR BLOCKING & COVARIATES ------------------------------------------------------------------------------------------#
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# Create the formula without Income_Level
formula_2 = 'Q("Conversion_Rate") ~ Q("Age") + C(Q("Membership_Status")) + Q("Total_Time_Spent_On_Platform") + C(Q("DiscountType")) + \
             Q("Age"):C(Q("DiscountType")) + C(Q("Membership_Status")):C(Q("DiscountType")) + Q("Total_Time_Spent_On_Platform"):C(Q("DiscountType")) + \
             Q("Age"):C(Q("Membership_Status")) + C(Q("Membership_Status")):Q("Total_Time_Spent_On_Platform") + Q("Total_Time_Spent_On_Platform"):Q("Age")'

# Fit the model
model_2 = sm.formula.ols(formula_2, data=df_parallel).fit()

# Perform ANOVA and print the table
aov_table = sm.stats.anova_lm(model_2)
print(aov_table)
print(model_2.summary())
print('\n')

#------------------------------------------------ TWO-WAY ANOVA FOR BLOCKING & COVARIATES END------------------------------------------------------------------------------------------#

# Perform Tukey's HSD for DiscountType
tukey_results = pairwise_tukeyhsd(
    df_parallel['Conversion_Rate'],  # Use the correct column for Conversion_Rate
    df_parallel['DiscountType']
)

print("\nTukey HSD Test Results:")
print(tukey_results)

"""**Checking Assumptions for COMBINED ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using


"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_2 = model_2.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_2, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_2, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_residual = stats.shapiro(residual_2.dropna())

print(f'W: {round(shapiro_residual[0], 3)}, P-value: {round(shapiro_residual[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_2 = model_2.resid
fitted_value = model_2.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_2, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""Develop a flowchart for parallel design"""

!pip install schemdraw
import schemdraw
from schemdraw import flow

with schemdraw.Drawing() as d:

    d.config(fontsize=11)
    b = flow.Start().label('Parallel Design: 146 Participants')
    flow.Arrow().right(d.unit/2).at(b.E)
    d0 = flow.Box(w=5, h=3.9).label('BLOCKING: \n Choosing Membership Status \n as the blocking factor \n we divided the \n 146 participants \n into close to \n four equal groups. \n Two groups of \n 36 participants \n and two groups of \n 37 participants.')
    d1 = flow.Box(w=5, h=3.9).label('RANDOMIZATION: \n The we randomly \n assigned each participant \n from each membership status \n into either the \n treatment (i.e. BOGOF) \n or the control (i.e. No Discount)\n groups. This gave \n us the required \n 73 participants per \n group and overall \n of 146 participants.')
    flow.Arrow().down(d.unit* 1).at(d1.S)
    d3 = flow.Decision(w=5.2, h=3.9).label('Group 2: 73 Participants')
    flow.Arrow().right(d.unit* 0.5).at(d1.E)
    d4 = flow.Decision(w=5.2, h=3.9).label('Group 1: 73 Participants')



    flow.Arrow().right(d.unit*1.5).at(d3.E)
    d6 = flow.Box(w=5.5, h=4.0 ).anchor('W').label('CONTROL GROUP: No Discount')
    flow.Arrow().right(d.unit*1.5).at(d4.E)
    d7 = flow.Box(w=6.9, h=4.0).anchor('W').label('TREATMENT GROUP: Buy One Get One Free')
    flow.Arrow().right(d.unit*1.5).at(d7.E)
    d8 = flow.Box(w=6.9, h=4.0).anchor('W').label('Apply Two-Way ANOVA \nto analyze the difference \nbetween treatment and control \ngroup with the blocking factor')
    flow.Arc2(k=-.4, arrow='->').at(d6.E).to(d8.S)

"""#--------------------- END OF PARALELL DESIGN CODE ----------------------------#

#**2. Cross-over Design**

**Section 2.1: Power Analysis, Sample Size, & Associated Graph** </br>
Effect Size: Medium (0.5)</br>
Sample Size: 76</br>
Dependent Variable: Average Order Value
"""

from statsmodels.stats.power import FTestAnovaPower
# Power Analysis for Cross Over Design Scenario
# Medium effect size
effect_size = 0.5
# Significance level
alpha = 0.05
# Power value
power = 0.85
# cross over design (2 groups)
number_groups = 2

# Creating a power analysis object
analysis = FTestAnovaPower()

# Calculating the required sample size per group
result = analysis.solve_power(effect_size=effect_size, k_groups=number_groups, alpha=alpha, power=power)

# Rounding to get integer sample size
required_sample_size_per_group = round(result)

# Total sample size required for the experiment
total_sample_size = required_sample_size_per_group * number_groups

# Output the result
print(f'Required sample size per group: {required_sample_size_per_group}')
print(f'Total sample size for all groups: {total_sample_size}')

# Power vs. number of observations
# Parameters for power analysis
effect_sizes = array([0.2, 0.5, 0.8]) # The larger the effect size, the less likely it is to be random error.
sample_sizes = array(range(2, 76))


# Calculate power curves from multiple power analyses.
# Assume a significance of 0.05 and explore the change in sample size between 2 and 100 with low (es=0.2), medium (es=0.5), and high effect (es=0.8)sizes.
analysis = FTestAnovaPower()
analysis.plot_power(dep_var='nobs', nobs=sample_sizes, effect_size=effect_sizes, alpha=0.05, title='Power of ANOVA Test')
pyplot.grid()
pyplot.ylabel('Power')
pyplot.show()

"""**Section 2.2: BLOCKING, TWO-WAY ANOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Blocking for crossover using Frequency in the past as blocking factor** <br>

> Frequent customer might be more willing to purchase, regardless of the promotion. <br>
Confounding Effect: If a specific promotion is used more often on frequent customer vs. another one on infrequent customer, the promotion's effect on average order value may be overestimated. <br>
"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING INCOME LEVELS -----------------------------------------------------------------------#
# add blocking factor for cross-over design
# Setting the seed for reproducibility
np.random.seed(42)

# Define columns to keep in the subset DataFrame
columns_to_keep = ['Participant ID', 'Age', 'Gender', 'Location', 'Income', 'Frequency in the Past']

# Randomly select 38 participants for each of the two device types
selected_participants_frequent = df[df['Frequency in the Past'] == 'frequent'].sample(n=38, random_state=42)[columns_to_keep]
selected_participants_not_frequent= df[df['Frequency in the Past'] == 'not frequent'].sample(n=38, random_state=42)[columns_to_keep]

# Combine the selected participants into one DataFrame
selected_participants_crossover = pd.concat([selected_participants_frequent, selected_participants_not_frequent])

# Display the first 10 entries of the combined selected participants DataFrame
selected_participants_crossover.head()

"""**Randomization for Cross Over Design After Blocking Done Above**"""

# Set seed for reproducibility
np.random.seed(42)

# Split each frequency group evenly into two groups
group1_frequent = selected_participants_frequent.iloc[:19, :]
group2_frequent = selected_participants_frequent.iloc[19:, :]
group1_not_frequent = selected_participants_not_frequent.iloc[:19, :]
group2_not_frequent = selected_participants_not_frequent.iloc[19:, :]

# Combine the split groups to form the full Group 1 and Group 2
group1 = pd.concat([group1_frequent, group1_not_frequent]).reset_index(drop=True)
group2 = pd.concat([group2_frequent, group2_not_frequent]).reset_index(drop=True)

# Create entries for each stage and treatment in each group
# Group 1 participants
group1_stage1 = pd.DataFrame(group1)
group1_stage1['Stage'] = 'Stage 1'
group1_stage1['Treatment'] = 'Percentage Markdown'

group1_stage2 = pd.DataFrame(group1)
group1_stage2['Stage'] = 'Stage 2'
group1_stage2['Treatment'] = 'Buy One Get One Free'

# Group 2 participants
group2_stage1 = pd.DataFrame(group2)
group2_stage1['Stage'] = 'Stage 1'
group2_stage1['Treatment'] = 'Buy One Get One Free'

group2_stage2 = pd.DataFrame(group2)
group2_stage2['Stage'] = 'Stage 2'
group2_stage2['Treatment'] = 'Percentage Markdown'

# Combine stages for each group
group1_crossover = pd.concat([group1_stage1, group1_stage2])
group2_crossover = pd.concat([group2_stage1, group2_stage2])

# Combine both groups after assignment
crossover_df = pd.concat([group1_crossover, group2_crossover]).sort_values(['Participant ID', 'Stage']).reset_index(drop=True)

# Simulate purchases with higher values for specific conditions
def simulate_order_values(row):
    mean = 325  # This is an example mean, halfway between 1 and 600
    std_dev = 100  # This is an example standard deviation
    base_value = np.random.normal(loc=mean, scale=std_dev)
    if row['Frequency in the Past'] == 'frequent' or row['Treatment'] == 'Buy One Get One Free':
        return base_value * 1.5  # Increase the order value by 50% for this subgroup
    else:
        return base_value



# Apply the function to the DataFrame
crossover_df['Average Order Value'] = crossover_df.apply(simulate_order_values, axis=1)

# Output the DataFrame for the first 10 participants as a sample
crossover_df.head(10)

"""**Violin Plot to Display the Distribution after the Blocking & Randomization for Cross Over Design**"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")

# Create a categorical plot with violin plot kind
g = sns.catplot(x="Stage", y="Average Order Value", hue="Treatment", col="Frequency in the Past",
                data=crossover_df, kind="violin", split=True, height=5, aspect=1,
                palette="muted", order=["Stage 1", "Stage 2"])

# Adding titles and labels
g.fig.suptitle('Violin Plot of Order Value by Crossover Groups Across Frequency Categories', fontsize=16, va='top')
g.set_axis_labels("Stage", "Average Order Value")
g.add_legend(title="Treatment")

# Adjusting subplots' spacing
g.fig.subplots_adjust(top=0.85)

# Displaying the plot
plt.show()

""">**Checking for Normality of Dependent Variable: Average Order Value**"""

from scipy import stats

# Perform Shapiro-Wilk test on Conversion_Rate and print result
shapiroUnem = stats.shapiro(crossover_df['Average Order Value'].dropna())

print(f'W: {round(shapiroUnem[0],3)}, P-value: {round(shapiroUnem[1],3)}')

# Create a histogram of the Average Order Value
plt.figure(figsize=(10, 6))  # Sets the figure size
plt.hist(crossover_df['Average Order Value'].dropna(), bins=30, color='blue', alpha=0.7)
plt.title('Distribution of Average Order Value')  # Adds a title to the histogram
plt.xlabel('Average Order Value')  # Label for the x-axis
plt.ylabel('Frequency')  # Label for the y-axis
plt.grid(True)  # Adds a grid for better readability
plt.show()  # Displays the plot

""">**TWO-WAY ANOVA & SUMMARY STATISTICS: After blocking using Frequency in the past and then randomly assigning each participant into two groups we use two-way ANOVA to find the significance of the blocking factor based on the p-value**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Define the formula for the model with control group ("No Discount") included in the 'Treatment Control' column of our dataframe
formula = 'Q("Average Order Value") ~ Q("Stage") + Q("Treatment") + Q("Frequency in the Past") + Q("Stage"):Q("Treatment") + Q("Stage"):Q("Frequency in the Past") + Q("Treatment"):Q("Frequency in the Past") + Q("Stage"):Q("Treatment"):Q("Frequency in the Past")'

# Fit the model using ordinary least squares (OLS)
model_crossover = ols(formula, data=crossover_df).fit()

# Perform the ANOVA and print the results
anova_table = sm.stats.anova_lm(model_crossover, typ=2)
print(anova_table)
print(model.summary())

"""**Perform POST HOC test after the TWO WAY ANOVA**"""

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_crossover = pairwise_tukeyhsd(
    crossover_df['Average Order Value'],
    crossover_df['Treatment']
)

print("\nTukey HSD Test Results:")
print(tukey_results_crossover)

"""**Checking Assumptions for Two-Way ANOVA:**
1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

residuals = model_crossover.resid
# Q-Q Plot to check for normality & heteroscedasticity
sm.qqplot(residuals, line='s')
plt.title('Q-Q Plot of Residuals')
plt.show()

"""**Section 2.3: CONTINUOUS COVARIATES, ANCOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Adding 1 Continuous Covariate. Using Age as the continuous covariate** <br>
"""

# Setting the seed for reproducibility: so that our analysis and conclusions consistent
np.random.seed(42)

# Select 76 unique random participants
unique_participants = df['Participant ID'].unique()
selected_participants = np.random.choice(unique_participants, 76, replace=False)

# Filter the DataFrame to keep only the selected participants
crossover_df = df[df['Participant ID'].isin(selected_participants)].copy()

#--------------------------------------------------------------------- Assignment 2: Adding a New Covariate --------------------------------------------------------------------------#
# Now only select the demographic features associated with each participant such as (Participant ID,	Age	Gender,	Location,	Income)
# Selecting the demographic features associated with each participant
demographic_features = ['Participant ID', 'Age', 'Gender', 'Location', 'Income']
# Filtering the DataFrame to include only the selected demographic features
# Also, making sure each participant is unique and only appears once
demographic_participant_df = crossover_df[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Now we must randomly assign 38 participants to each of the 2 groups:
# Group 1: Stage1 Percentage Markdown + washout period + Stage2 Buy One Get One Free
# Group 2: Stage1 Buy One Get One Free + washout period + Stage2 Percentage Markdown

# Defining the group combinations based on above cross over design
crossover_conditions = [
    # Group 1 Stage 1: Stage1 Percentage Markdown
    ('Stage1', 'Percentage Markdown'),
    # Group 1 Staeg 2: Stage2 Buy One Get One Free
    ('Stage2', 'Buy One Get One Free'),
    # Group 2: Stage 1: Stage1 Buy One Get One Free
    ('Stage1', 'Buy One Get One Free'),
    # Group 2: Stage 2: Stage2 Percentage Markdown
    ('Stage2', 'Percentage Markdown')]

# There will be 2 groups
num_groups = 2
# Based on the power analysis we will evenly split 38 individuals per group
group_size = 38

# Randomly assign participants to two groups by shuffling the IDs
np.random.shuffle(demographic_participant_df['Participant ID'].values)

# Initializing arrays for treatment and stage assignments
stages = []
treatments = []

# Assign treatments and stages based on the crossover design
for i in range(len(demographic_participant_df)):
    if i < len(demographic_participant_df) / 2:
        # First half of the participants for Group 1
        stages += ['Stage 1', 'Stage 2']
        treatments += ['Percentage Markdown', 'Buy One Get One Free']
    else:
        # Second half for Group 2
        stages += ['Stage 1', 'Stage 2']
        treatments += ['Buy One Get One Free', 'Percentage Markdown']

# Duplicate each row in the demographic DataFrame for the crossover stages
crossover_df = pd.concat([demographic_participant_df]*2).sort_values('Participant ID').reset_index(drop=True)

# Assign stages and treatments to the crossover DataFrame
crossover_df['Stage'] = stages
crossover_df['Treatment'] = treatments

# Simulating purchases
n_samples = len(crossover_df)
average_order_value = np.random.uniform(20, 500, n_samples).round(2)

# Add purchase amount data to the DataFrame
crossover_df['Average Order Value'] = average_order_value

# Output the DataFrame for the first 10 participants as a sample
crossover_df.head()

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Age'is the continuous covariates
Stage and Treatment will be the categorical variable (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

continuous_covariate = ['Age']
# Define the formula for the model with control group ("No Discount") included in the 'Treatment Control' column of our dataframe
formula = 'Q("Average Order Value") ~ Q("Age") + C(Q("Treatment")) + C(Q("Stage")) + \
          Q("Age"):C(Q("Treatment")) + Q("Age"):C(Q("Stage")) + \
          C(Q("Treatment")):C(Q("Stage"))'

# Fit the model using ordinary least squares (OLS)
model_crossover_c = ols(formula, data=crossover_df).fit()

# Perform the ANCOVA and print the results
# Use typ=3 for Type III SS: used in ANCOVA because it tests the main effects and interactions, adjusting for the presence of other factors
anova_table = sm.stats.anova_lm(model_crossover_c, typ=3)
print(anova_table)
print(model_crossover_c.summary())

"""**Perform POST HOC Test after ANCOVA**"""

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_crossover = pairwise_tukeyhsd(
    crossover_df['Average Order Value'],
    crossover_df['Treatment']
)

print("\nTukey HSD Test Results:")
print(tukey_results_crossover)

"""**Checking Assumptions for ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

# Get residuals after fitting the model
residual_crossover_c = model_crossover_c.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_crossover_c, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_crossover_c, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_resid = stats.shapiro(residual_crossover_c.dropna())

print(f'W: {round(shapiro_resid[0], 3)}, P-value: {round(shapiro_resid[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_crossover_c = model_crossover_c.resid
fitted_value = model_crossover_c.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_crossover_c, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Section 2.4: COMBINING BLOCKING & CONTINUOUS COVARIATES, ANCOVA FOR BOTH, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - BONUS PART: Running another set of two-way ANOVAs with both the blocking factor and continuous covariates for crossover design subset of data.** <br>
"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING INCOME LEVELS -----------------------------------------------------------------------#
# add blocking factor for cross-over design
# Setting the seed for reproducibility
np.random.seed(42)

# Define columns to keep in the subset DataFrame
columns_to_keep = ['Participant ID', 'Age', 'Gender', 'Location', 'Income', 'Frequency in the Past']

# Randomly select 38 participants for each of the two device types
selected_participants_frequent = df[df['Frequency in the Past'] == 'frequent'].sample(n=38, random_state=42)[columns_to_keep]
selected_participants_not_frequent= df[df['Frequency in the Past'] == 'not frequent'].sample(n=38, random_state=42)[columns_to_keep]

# Combine the selected participants into one DataFrame
selected_participants_crossover = pd.concat([selected_participants_frequent, selected_participants_not_frequent])

# Display the first 10 entries of the combined selected participants DataFrame
selected_participants_crossover.head()

# Set seed for reproducibility
np.random.seed(42)

# Split each frequency group evenly into two groups
group1_frequent = selected_participants_frequent.iloc[:19, :]
group2_frequent = selected_participants_frequent.iloc[19:, :]
group1_not_frequent = selected_participants_not_frequent.iloc[:19, :]
group2_not_frequent = selected_participants_not_frequent.iloc[19:, :]

# Combine the split groups to form the full Group 1 and Group 2
group1 = pd.concat([group1_frequent, group1_not_frequent]).reset_index(drop=True)
group2 = pd.concat([group2_frequent, group2_not_frequent]).reset_index(drop=True)

# Create entries for each stage and treatment in each group
# Group 1 participants
group1_stage1 = pd.DataFrame(group1)
group1_stage1['Stage'] = 'Stage 1'
group1_stage1['Treatment'] = 'Percentage Markdown'

group1_stage2 = pd.DataFrame(group1)
group1_stage2['Stage'] = 'Stage 2'
group1_stage2['Treatment'] = 'Buy One Get One Free'

# Group 2 participants
group2_stage1 = pd.DataFrame(group2)
group2_stage1['Stage'] = 'Stage 1'
group2_stage1['Treatment'] = 'Buy One Get One Free'

group2_stage2 = pd.DataFrame(group2)
group2_stage2['Stage'] = 'Stage 2'
group2_stage2['Treatment'] = 'Percentage Markdown'

# Combine stages for each group
group1_crossover = pd.concat([group1_stage1, group1_stage2])
group2_crossover = pd.concat([group2_stage1, group2_stage2])

# Combine both groups after assignment
crossover_df = pd.concat([group1_crossover, group2_crossover]).sort_values(['Participant ID', 'Stage']).reset_index(drop=True)

# Simulate purchases with higher values for specific conditions
def simulate_order_values(row):
    mean = 325  # This is an example mean, halfway between 1 and 600
    std_dev = 100  # This is an example standard deviation
    base_value = np.random.normal(loc=mean, scale=std_dev)
    if row['Frequency in the Past'] == 'frequent' or row['Treatment'] == 'Buy One Get One Free':
        return base_value * 1.5  # Increase the order value by 50% for this subgroup
    else:
        return base_value



# Apply the function to the DataFrame
crossover_df['Average Order Value'] = crossover_df.apply(simulate_order_values, axis=1)

# Output the DataFrame for the first 10 participants as a sample
crossover_df.head(10)

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Age' is the continuous covariates, Frequency in the past will be a categorical blocking factor, and Stage and Treatment will be the categorical variable (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Assuming 'df' is your DataFrame already loaded with the necessary data

# Create model
formula = 'Q("Average Order Value") ~ Q("Age") + C(Q("Frequency in the Past")) + C(Q("Stage")) + C(Q("Treatment")) + \
           Q("Age"):C(Q("Stage")) + Q("Age"):C(Q("Treatment")) + \
           C(Q("Frequency in the Past")):C(Q("Stage")) + C(Q("Frequency in the Past")):C(Q("Treatment")) + \
           C(Q("Stage")):C(Q("Treatment")) + \
           Q("Age"):C(Q("Frequency in the Past"))'

# Fit the model using ordinary least squares (OLS)
model_crossover_2 = sm.formula.ols(formula, data=crossover_df).fit()

# Perform ANOVA and print the table
anova_table = sm.stats.anova_lm(model_crossover_2, typ=2)  # Using Type II SS
print(anova_table)

# Print a detailed summary of the model
print(model_crossover_2.summary())

"""**Perform POST HOC Tukey's HSD test after TWO WAY ANOVA test**"""

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_crossover = pairwise_tukeyhsd(
    crossover_df['Average Order Value'],
    crossover_df['Treatment']
)

print("\nTukey HSD Test Results:")
print(tukey_results_crossover)

"""**Checking Assumptions for COMBINED ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_crossover_2 = model_crossover_2.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_crossover_2, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_crossover_2, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_residual = stats.shapiro(residual_crossover_2.dropna())

print(f'W: {round(shapiro_residual[0], 3)}, P-value: {round(shapiro_residual[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_crossover_2 = model_crossover_2.resid
fitted_value = model_crossover_2.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_crossover_2, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Flowchart for crossover design with the blocking factor**"""

with schemdraw.Drawing() as d:

    d.config(fontsize=11)
    b = flow.Start().label('Cross Over Design')
    flow.Arrow().right(d.unit/2).at(b.E)
    d1 = flow.Box(w=7, h=3.9).label('Blocking factor: Frequency in the past \n frequent \n not frequent')
    flow.Arrow().right(d.unit* 1).at(d1.E)
    d1 = flow.Box(w=7, h=3.9).label('Randomization: \n We select 76 participants with half \nfrequent and half non-frequent users\n for the experiment according \n to power analysis\n Then, we evenly split the participants \ninto two groups to proceed \nwith cross-over design')
    flow.Arrow().up(d.unit* 1).at(d1.N)
    d2 = flow.Box(w=5, h=3.9).label('Group 1')
    flow.Arrow().down(d.unit* 1).at(d1.S)
    d3 = flow.Box(w=5.2, h=3.9).label('Group 2')

    flow.Arrow().right(d.unit*1.5).at(d2.N)
    d5 = flow.Box(w=5.5, h=2.0).anchor('W').label('Treatment1: Percentage Markdown \n Frequent user in the past')
    flow.Arrow().right(d.unit*1.5).at(d3.N)
    d6 = flow.Box(w=5.5, h=2.0 ).anchor('W').label('Treatment2: Buy One Get One Free \n Frequent user in the past')

    flow.Arrow().right(d.unit*1.5).at(d2.S)
    d10 = flow.Box(w=5.5, h=2.0).anchor('W').label('Treatment1: Percentage Markdown \n Not frequent user in the past')

    flow.Arrow().right(d.unit*1.5).at(d3.S)
    d11 = flow.Box(w=5.5, h=2.0 ).anchor('W').label('Treatment2: Buy One Get One Free \n Not frequent user in the past')


    flow.Arrow().right(d.unit*1.5).at(d5.E).label('Washout Period: 2 Weeks')
    d5 = flow.Box(w=5.5, h=2.0).anchor('W').label('Treatment2: Buy One Get One Free \n Frequent user in the past')

    flow.Arrow().right(d.unit*1.5).at(d10.E).label('Washout Period: 2 Weeks')
    d5 = flow.Box(w=5.5, h=2.0).anchor('W').label('Treatment2: Buy One Get One Free \n Not frequent user in the past')

    flow.Arrow().right(d.unit*1.5).at(d6.E).label('Washout Period: 2 Weeks')
    d6 = flow.Box(w=5.5, h=2.0 ).anchor('W').label('Treatment1: Percentage Markdown \n Frequent user in the past')

    flow.Arrow().right(d.unit*1.5).at(d11.E).label('Washout Period: 2 Weeks')
    d6 = flow.Box(w=5.5, h=2.0 ).anchor('W').label('Treatment1: Percentage Markdown \n Not frequent user in the past')

"""# --------------------- END OF CROSSOVER DESIGN CODE ----------------------------

#**3. Withdrawal Design** <br>

**Section 3.1: Power Analysis, Sample Size, & Associated Graph** </br>
Effect Size: Small (0.5)</br>
Power: 0.85 <br>
Sample Size: 76</br>
dependent variable: session duration
"""

# Power Analysis for Withdrawal Design Scenario
# Medium effect size
effect_size = 0.5
# Significance level
alpha = 0.05
# Power value
power = 0.85
# withdrawal design (2 groups)
number_groups = 2

# Creating a power analysis object
analysis = FTestAnovaPower()

# Calculating the required sample size per group
result = analysis.solve_power(effect_size=effect_size, k_groups=number_groups, alpha=alpha, power=power)

# Rounding to get integer sample size
required_sample_size_per_group = round(result)

# Total sample size required for the experiment
total_sample_size = required_sample_size_per_group * number_groups

# Output the result
print(f'Required sample size per group: {required_sample_size_per_group}')
print(f'Total sample size for all groups: {total_sample_size}')

# Power vs. number of observations
# Parameters for power analysis
effect_sizes = array([0.2, 0.5, 0.8]) # The larger the effect size, the less likely it is to be random error.
sample_sizes = array(range(2, 76))


# Calculate power curves from multiple power analyses.
# Assume a significance of 0.05 and explore the change in sample size between 2 and 100 with low (es=0.2), medium (es=0.5), and high effect (es=0.8)sizes.
analysis = FTestAnovaPower()
analysis.plot_power(dep_var='nobs', nobs=sample_sizes, effect_size=effect_sizes, alpha=0.05, title='Power of ANOVA Test')
pyplot.grid()
pyplot.ylabel('Power')
pyplot.show()

"""**Section 3.2: BLOCKING, TWO-WAY ANOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Blocking for withdrawal design using Device Type as blocking factor** <br>

> Mobile user customers might be more willing to stay longer to browse item and purchase online, regardless of the promotion. <br>
Confounding Effect: If a specific promotion is used more often on mobile user vs. another one one desktop user, the promotion's effect on session duration may be overestimated. <br>
"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING INCOME LEVELS -----------------------------------------------------------------------#
# Setting the seed for reproducibility
np.random.seed(42)

# Define the device types of interest (assuming Mobile and Desktop are the device types of interest)
device_types_of_interest = ['Mobile', 'Desktop']

# Filtering the DataFrame for the device types of interest
filtered_df = df[df['Device Type'].isin(device_types_of_interest)]

# Define columns to keep in the subset DataFrame
columns_to_keep = ['Participant ID', 'Age', 'Gender', 'Location', 'Income', 'Device Type']

# Randomly select 38 participants for each of the two device types
selected_participants_mobile = filtered_df[filtered_df['Device Type'] == 'Mobile'].sample(n=38, random_state=42)[columns_to_keep]
selected_participants_desktop = filtered_df[filtered_df['Device Type'] == 'Desktop'].sample(n=38, random_state=42)[columns_to_keep]

# Combine the selected participants into one DataFrame
selected_participants = pd.concat([selected_participants_mobile, selected_participants_desktop])

# Display the first 10 entries of the combined selected participants DataFrame
selected_participants.head()

"""**Randomization for Withdrawal Design After Blocking Done Above**"""

# Defining the t based on design
withdrawal_conditions = ['Control', 'Treatment']

# Shuffle within each device type and assign 'Control' or 'Treatment' evenly
def assign_treatments(group):
    treatments = ['Control', 'Treatment'] * (len(group) // 2)  # This creates an equal number of 'Control' and 'Treatment'
    np.random.shuffle(treatments)
    group['Treatment'] = treatments
    return group

selected_participants = selected_participants.groupby('Device Type').apply(assign_treatments).reset_index(drop=True)

# Simulating session duration
selected_participants['Session Duration'] = np.where(
    selected_participants['Treatment'] == 'Treatment',
    # Longer duration for treatment group
    np.random.uniform(500, 1000, len(selected_participants)),
    # Shorter duration for control group
    np.random.uniform(300, 800, len(selected_participants))
).round(2)

# Output the DataFrame for the first 10 participants as a sample
selected_participants.head()

""">**Violin Plot to Display the Distribution after the Blocking & Randomization for Withdrawal Design**"""

# Plotting the violin plot for session duration
plt.figure(figsize=(10, 6))
sns.violinplot(x="Treatment", y="Session Duration", data=selected_participants, order = ['Control', 'Treatment'], hue="Device Type",legend=True)

# Adding the titles and labels to the plot
plt.title('Violin Plot of Order Value by withdrawal design stages', fontsize=16)
plt.xlabel('Treatment', fontsize=12)
plt.ylabel('Session Duration', fontsize=12)
plt.legend(title='Treatment', loc='center left', bbox_to_anchor=(1, 0.5))

# Displaying the plot
plt.show()

"""**Checking for Normality of Dependent Variable: Session Duration**"""

from scipy import stats

# Perform Shapiro-Wilk test on Conversion_Rate and print result
shapiroUnem = stats.shapiro(selected_participants['Session Duration'].dropna())

print(f'W: {round(shapiroUnem[0],3)}, P-value: {round(shapiroUnem[1],3)}')

""">**TWO-WAY ANOVA & SUMMARY STATISTICS: After blocking using Device Type and then randomly assigning each participant into two groups we use two-way ANOVA to find the significance of the blocking factor based on the p-value**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Define the formula for the model with control group ("No Discount") included in the 'Treatment Control' column of our dataframe
formula = 'Q("Session Duration") ~ Q("Treatment") + Q("Device Type") + \
          Q("Age") + Q("Treatment"):Q("Device Type") + \
          Q("Age"):Q("Treatment") + Q("Age"):Q("Device Type")'

# Fit the model using ordinary least squares (OLS)
model_withdrawal = ols(formula, data=selected_participants).fit()

# Perform the ANOVA and print the results
anova_table = sm.stats.anova_lm(model_withdrawal, typ=2)
print(anova_table)
print(model_withdrawal.summary())

"""**Perform POST HOC TUKEY"S HSD test after TWO WAY ANOVA**"""

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_withdrawal = pairwise_tukeyhsd(
    selected_participants['Session Duration'],
    selected_participants['Treatment']
)

print("\nTukey HSD Test Results:")
print(tukey_results_withdrawal)

"""**Checking Assumptions for Two-Way ANOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residuals_withdrawal = model_withdrawal.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residuals_withdrawal, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residuals_withdrawal, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

""">**Performing Shapiro-Wilk test on residuals after fitting the model to check for assumption of normality.**

> The residuals of our model seem to follow a normal distribution, as the Shapiro-Wilk test results indicate there is no significant deviation from normality. It implies that the assumptions of normality for our residuals are likely met, which is an important condition for fitting our model.
"""

# Perform Shapiro-Wilk test on residuals
shapiro_res = stats.shapiro(residuals_withdrawal.dropna())

print(f'W: {round(shapiro_res[0], 3)}, P-value: {round(shapiro_res[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residuals_withdrawal = model_withdrawal.resid
fitted_values = model_withdrawal.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_values, y=residuals_withdrawal, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Section 3.3: CONTINUOUS COVARIATES, ANCOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Adding 1 Continuous Covariate (Income)** <br>
"""

#--------------------------------------------------------------------- Assignment 2: Adding a New Covariate --------------------------------------------------------------------------#
# Select 76 unique random participants
selected_participant_ids = np.random.choice(df['Participant ID'].unique(), 76, replace=False)
withdrawal_df_c = df[df['Participant ID'].isin(selected_participant_ids)].copy()

# Define the columns you want to keep
columns_to_keep = ['Participant ID', 'Age', 'Gender', 'Location', 'Income']

# Filter the DataFrame to include only the specified columns
withdrawal_df_c = withdrawal_df_c[columns_to_keep]

# Initial stage: all participants receive "Percentage Markdown"
withdrawal_df_c['Stage 1 Session Duration'] = np.random.uniform(500, 1000, len(withdrawal_df_c)).round(2)

# Assign participants to two groups: Treatment (continues with promotion) and Control (no promotion in Stage 2)
group_assignments = np.random.choice(['Control', 'Treatment'], size=len(withdrawal_df_c), replace=True, p=[0.5, 0.5])
withdrawal_df_c['Group'] = group_assignments

# Second stage: Treatment group continues with "Percentage Markdown", Control does not
withdrawal_df_c['Stage 2 Session Duration'] = np.where(
    withdrawal_df_c['Group'] == 'Treatment',
    np.random.uniform(500, 1000, len(withdrawal_df_c)),  # Longer duration for continuing promotion
    np.random.uniform(300, 800, len(withdrawal_df_c))     # Shorter duration for no promotion
).round(2)

# Output the DataFrame for the first 10 participants as a sample
withdrawal_df_c.head(10)

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Income' is the continuous covariates
Treatment(Group) will be the categorical variable (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Assuming 'withdrawal_df_c' is the DataFrame that contains all necessary data
# Update the formula to focus on the independent variables and their interactions, excluding 'Device Type'
formula = 'Q("Stage 2 Session Duration") ~ Q("Income") + C(Q("Group")) + \
          Q("Income"):C(Q("Group"))'

# Fit the model using ordinary least squares (OLS)
model_withdrawal_c = ols(formula, data=withdrawal_df_c).fit()

# Perform the ANOVA and print the results
anova_table = sm.stats.anova_lm(model_withdrawal_c, typ=3)
print(anova_table)

# Print a detailed summary of the model to see coefficients, standard errors, p-values, and more
print(model_withdrawal_c.summary())

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_withdrawal_c = pairwise_tukeyhsd(
    withdrawal_df_c['Stage 2 Session Duration'],
    withdrawal_df_c['Group']
)

print("\nTukey HSD Test Results:")
print(tukey_results_withdrawal_c)

"""**Checking Assumptions for ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using

"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_withdrawal_c = model_withdrawal_c.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_withdrawal_c, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_withdrawal_c, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_resid = stats.shapiro(residual_withdrawal_c.dropna())

print(f'W: {round(shapiro_resid[0], 3)}, P-value: {round(shapiro_resid[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_withdrawal_c = model_withdrawal_c.resid
fitted_value = model_withdrawal_c.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_withdrawal_c, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Section 3.4: COMBINING BLOCKING & CONTINUOUS COVARIATES, ANCOVA FOR BOTH, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - BONUS PART: Running another set of two-way ANOVAs with both the blocking factor and continuous covariates for withdrawal design subset of data.** <br>
"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING INCOME LEVELS -----------------------------------------------------------------------#
# Setting the seed for reproducibility
np.random.seed(42)

# Define the device types of interest (assuming Mobile and Desktop are the device types of interest)
device_types_of_interest = ['Mobile', 'Desktop']

# Filtering the DataFrame for the device types of interest
filtered_df = df[df['Device Type'].isin(device_types_of_interest)]

#--------------------------------------------------------------------- Assignment 2: Adding a New Covariate --------------------------------------------------------------------------#
# Define columns to keep in the subset DataFrame
columns_to_keep = ['Participant ID', 'Age', 'Gender', 'Location', 'Income', 'Device Type']

# Randomly select 38 participants for each of the two device types
selected_participants_mobile = filtered_df[filtered_df['Device Type'] == 'Mobile'].sample(n=38, random_state=42)[columns_to_keep]
selected_participants_desktop = filtered_df[filtered_df['Device Type'] == 'Desktop'].sample(n=38, random_state=42)[columns_to_keep]

# Combine the selected participants into one DataFrame
selected_participants = pd.concat([selected_participants_mobile, selected_participants_desktop])

# Defining the t based on design
withdrawal_conditions = ['Control', 'Treatment']

# Shuffle within each device type and assign 'Control' or 'Treatment' evenly
def assign_treatments(group):
    treatments = ['Control', 'Treatment'] * (len(group) // 2)  # This creates an equal number of 'Control' and 'Treatment'
    np.random.shuffle(treatments)
    group['Treatment'] = treatments
    return group

selected_participants = selected_participants.groupby('Device Type').apply(assign_treatments).reset_index(drop=True)

# Simulating session duration
selected_participants['Session Duration'] = np.where(
    selected_participants['Treatment'] == 'Treatment',
    # Longer duration for treatment group
    np.random.uniform(500, 1000, len(selected_participants)),
    # Shorter duration for control group
    np.random.uniform(300, 800, len(selected_participants))
).round(2)

# Output the DataFrame for the first 10 participants as a sample
selected_participants.head()

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> "Income" is the continuous covariates, Device Type will be a categorical blocking factor, and Treatment/Control will be the categorical variable (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

#------------------------------------------------ TWO-WAY ANOVA FOR BLOCKING & COVARIATES ------------------------------------------------------------------------------------------#
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Assuming 'df' is your DataFrame already loaded with the necessary data
# Create model formula
formula = 'Q("Session Duration") ~ Q("Income") + C(Q("Device Type")) + C(Q("Treatment")) + \
           Q("Income"):C(Q("Device Type")) + Q("Income"):C(Q("Treatment")) + \
           C(Q("Device Type")):C(Q("Treatment"))'

# Fit the model using ordinary least squares (OLS)
model_withdrawal_2 = sm.formula.ols(formula, data=selected_participants).fit()

# Perform ANOVA and print table
aov_table = sm.stats.anova_lm(model_withdrawal_2, typ=2)  # Using Type II sums of squares
print(aov_table)

# Print a detailed summary of the model
print(model_withdrawal_2.summary())
#------------------------------------------------ TWO-WAY ANOVA FOR BLOCKING & COVARIATES END---------------------------------------------------------------

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_withdrawal_2 = pairwise_tukeyhsd(
    selected_participants['Session Duration'],
    selected_participants['Treatment']
)

print("\nTukey HSD Test Results:")
print(tukey_results_withdrawal_2)

"""**Checking Assumptions for COMBINED ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_withdrawal_2 = model_withdrawal_2.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_withdrawal_2, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_withdrawal_2, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_residual = stats.shapiro(residual_withdrawal_2.dropna())

print(f'W: {round(shapiro_residual[0], 3)}, P-value: {round(shapiro_residual[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_withdrawal_2 = model_withdrawal_2.resid
fitted_value = model_withdrawal_2.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_withdrawal_2, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""The Flowchart for Withdrawal Design with the blocking factor(Device Type)"""

with schemdraw.Drawing() as d:

    d.config(fontsize=11)
    b = flow.Start().label('Withdrawal Design')
    flow.Arrow().right(d.unit/2).at(b.E)
    d10 = flow.Box(w=7, h=3.9).label('Blocking Factor: Device Type \n Mobile Users \n Desktop Users')
    flow.Arrow().right(d.unit* 1).at(d10.E)
    d1 = flow.Box(w=10, h=7.5).label('Randomization: \n We randomly select 76 participants \n by stratifying blocking factor \n (device type) \n From week1 to week4 \n All Participants receive PM promotion \n  Starting at week5 \n control groups do not receive promotion \n Treatment groups continue receiving PM promotion')

    flow.Arrow().up(d.unit* 1).at(d1.N)
    d2 = flow.Box(w=5, h=3.9).label('Group 1')
    flow.Arrow().down(d.unit* 1).at(d1.S)
    d3 = flow.Box(w=5.2, h=3.9).label('Group 2')

    flow.Arrow().right(d.unit*2).at(d2.E)
    d5 = flow.Box(w=5.5, h=2.0).label('Percentage Markdown')



    flow.Arrow().right(d.unit*2).at(d3.E)
    d6 = flow.Box(w=5.5, h=2.0 ).anchor('W').label('Percentage Markdown')


    flow.Arrow().right(d.unit*1.5).at(d5.E).label('4 weeks after')
    d7 = flow.Box(w=5.5, h=2.0).anchor('W').label('Treatment: Continue Receiving \nPercentage Markdown')



    flow.Arrow().right(d.unit*1.5).at(d6.E).label('4 weeks after')
    d8 = flow.Box(w=5.5, h=2.0 ).anchor('W').label('Withdrawal: No Promotion')

"""# --------------------- END OF WITHDRAWAL DESIGN CODE ----------------------------

#**4. Factorial Design** <br>

**Section 4.1: Power Analysis, Sample Size, & Associated Graph**

Effect Size: Medium (0.5)</br>
Sample Size: 212</br>
Dependent Variable: Conversion Rate <br>

![image.png](data:image/png;base64,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)
"""

from statsmodels.stats.power import FTestAnovaPower

# Define the parameters for this Factorial Design Scenario

# Medium effect size
effect_size = 0.5
# Significance level
alpha = 0.05
# Power value
power = 0.85
# 2x2 factorial design (4 groups)
number_groups = 4

# Creating a power analysis object
analysis = FTestAnovaPower()

# Calculating the required sample size per group
result = analysis.solve_power(effect_size=effect_size, k_groups=number_groups, alpha=alpha, power=power)

# Rounding to get integer sample size
required_sample_size_per_group = round(result)

# Total sample size required for the experiment
total_sample_size = required_sample_size_per_group * number_groups

# Output the result
print(f'Required sample size per group: {required_sample_size_per_group}')
print(f'Total sample size for all groups: {total_sample_size}')

from numpy import array
from matplotlib import pyplot

# Power vs. number of observations
# Parameters for power analysis
# The larger the effect size, the less likely it is to be random error.
effect_sizes = array([0.2, 0.5, 0.8])
sample_sizes = array(range(2, 212))


# Calculate power curves from multiple power analyses.
# Assume a significance of 0.05 and explore the change in sample size between 2 and 100 with low (es=0.2), medium (es=0.5), and high effect (es=0.8)sizes.
analysis = FTestAnovaPower()
analysis.plot_power(dep_var='nobs', nobs=sample_sizes, effect_size=effect_sizes, alpha=0.05, title='Power of ANOVA Test')
pyplot.grid()
pyplot.ylabel('Power')
pyplot.show()

"""**Assignment 2 - Blocking Factor for Factorial Design**

Customers in big or samll cities may have different reactions for holiday seasons with or without promotions.
Confounding Effect: Holidays or Promotions, customers in big cities are more likely to have purchases than in small cities.

**Section 4.2: BLOCKING, TWO-WAY ANOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Blocking for Factorial using City Type as blocking factor** <br>

> Customers in big or samll cities may have different reactions for holiday seasons with or without promotions. <br>
Confounding Effect: Holidays or Promotions, customers in big cities are more likely to have purchases than in small cities. <br>
"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING City Size -----------------------------------------------------------------------#
# add blocking factor for factorial design
# Setting the seed for reproducibility# Setting the seed for reproducibility: so that our analysis and conclusions consistent
np.random.seed(42)

# Define population for each city
city_populations = {
    'Toronto': 2600000, 'Montreal': 1600000, 'Vancouver': 600000,
    'Calgary': 1020000, 'Edmonton': 710000, 'Ottawa': 810000,
    'Winnipeg': 630000, 'Quebec City': 530000, 'Hamilton': 600000,
    'Kitchener': 230000, 'London': 350000, 'Victoria': 290000,
    'Halifax': 360000, 'Oshawa': 250000, 'Windsor': 280000,
    'Saskatoon': 200000, 'Regina': 180000, 'St. John\'s': 100000,
    'Kelowna': 130000, 'Barrie': 180000
}


# Assign population for each city in the 'Location' column
df['City Population'] = df['Location'].map(city_populations)

# Create a new column for city population level
df['City_type'] = ''

# Loop through each row to categorize the city Population
for index, row in df.iterrows():
    if row['City Population'] >= 500000:
        df.at[index, 'City_type'] = 'Big City'
    elif row['City Population'] < 500000:
        df.at[index, 'City_type'] = 'Small City'

# Create a dictionary to store the participants by city population level
city_samples = {}
# Create labels to categorize each of the city popualtion per participant
city_label = ['Big City', 'Small City']

# Looping through each city popualtion level and randomly select 106 participants
for City_type in city_label:
    selected_city = df[df['City_type'] == City_type].sample(n=106, random_state=42)
    city_samples[City_type] = selected_city

# Combine all selected participants from the dictionary into one DataFrame
selected_participants = pd.concat(city_samples.values())

# Find remaining participants who were not selected
remaining_participants = df[~df['Participant ID'].isin(selected_participants['Participant ID'])]

# Count how many participants are in each level
city_level_counts = selected_participants['City_type'].value_counts()

# Print out the number of participants in each income level & display the dataframe
print(city_level_counts)
print('\n')
selected_participants.head()

"""**Randomdization After Blocking Factor After Blocking Done Above**"""

# Set seed for reproducibility
np.random.seed(42)

# Filter the DataFrame to keep only the selected participants
factorial_df = df[df['Participant ID'].isin(selected_participants)].copy()

# Separate participants by City_type
big_city_participants = selected_participants[selected_participants['City_type'] == 'Big City']
small_city_participants = selected_participants[selected_participants['City_type'] == 'Small City']

# We will randomly assign participants from both groups to the 4 groups evenly.
# Randomly assign participants from Big City to 4 groups
big_city_participants['Group'] = np.random.choice(['Group 1', 'Group 2', 'Group 3', 'Group 4'],
                                                   size=big_city_participants.shape[0], replace=True)

# Randomly assign participants from Small City to 4 groups
small_city_participants['Group'] = np.random.choice(['Group 1', 'Group 2', 'Group 3', 'Group 4'],
                                                     size=small_city_participants.shape[0], replace=True)

# Combine the Big City and Small City participants back together
final_selected_participants = pd.concat([big_city_participants, small_city_participants])

# Now we create the demographic and factorial data for these participants
# Filter the original df for demographic features of selected participants
demographic_features = ['Participant ID', 'Age', 'Gender', 'Location', 'Income','Group', 'City_type']
demographic_participant_df = final_selected_participants[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Now we must randomly assign 53 participants to each of the four groups:
# Group1: Holiday + Percentage Markdown
# Group 2: Holiday + No Discount
# Group 3: No Holiday + Percentage Markdown
# Group 4: No Holiday + No Discount
# Defining the group combinations based on above factorial design
factorial_conditions = [
    # Group 1: Holiday + Percentage Markdown
    ('Yes', 'Percentage Markdown'),
    # Group 2: Holiday + No Discount
    ('Yes', 'No Discount'),
    # Group 3: No Holiday + Percentage Markdown
    ('No', 'Percentage Markdown'),
    # Group 4: No Holiday + No Discount
    ('No', 'No Discount')]

# There will be 4 groups
num_groups = len(factorial_conditions)
# Based on the power analysis we will evenly split 53 individuals per group
group_size = len(demographic_participant_df) // num_groups

# Initializing two lists to store assigned factor levels
holiday_assignments = []
discount_assignments = []

# Assigning d groups of participants to factorial groups using a loop
for index, row in demographic_participant_df.iterrows():
    # Get the group assigned to the participant
    group = row['Group']
    # Map the group to the corresponding factorial condition (Holiday, Discount)
    if group == 'Group 1':
        holiday, discount = factorial_conditions[0]
    elif group == 'Group 2':
        holiday, discount = factorial_conditions[1]
    elif group == 'Group 3':
        holiday, discount = factorial_conditions[2]
    elif group == 'Group 4':
        holiday, discount = factorial_conditions[3]
    # Append the corresponding holiday and discount values
    holiday_assignments.append(holiday)
    discount_assignments.append(discount)

# Adding the factor assignments to the DataFrame
demographic_participant_df['Holiday'] = holiday_assignments
demographic_participant_df['Discount'] = discount_assignments

# Now we will generate the dependent variables for this experiment specifically
n_samples = len(demographic_participant_df)

# Initializing activity data and using zeros as placeholders
frequency_of_visits = np.zeros(n_samples)
frequency_of_made_purchase = np.zeros(n_samples)

# Looping through each of the rows
for i in range(n_samples):
  # holds the holiday status ('Yes' or 'No') for the current participant
  holiday = holiday_assignments[i]
  # holds the discount type ('Percentage Markdown' or 'No Discount') for the current participant
  discount = discount_assignments[i]

  # For the following condition (i.e. the holiday variable is set to yes)
  if holiday == 'Yes':
    # Then we have to anticipate an increase in the frequency of visits from participants
    # So we create our dependent variables accordingly
    frequency_of_visits[i] = np.random.randint(10, 30)
  else:
    # If there are no holidays the traffic to the platform should be less
    # so we reflect that in the two dependent variables
    frequency_of_visits[i] = np.random.randint(5, 20)

  # For the following condition (i.e. Percentage Markdown is applied)
  # There will be Higher purchases and order value when discounts are applied
  if discount == 'Percentage Markdown':
    # Making sure that the purchases don't exceed the number of visits to solve the
    # case when there are zero visits there should be 0 purchases
    frequency_of_made_purchase[i] = np.random.randint(0, frequency_of_visits[i])
  else:
    # Making sure that the purchases don't exceed the number of visits
    frequency_of_made_purchase[i] = np.random.randint(0, frequency_of_visits[i])


# Calculating conversion rate: which is a just the purchases made to visits
conversion_rate = frequency_of_made_purchase / frequency_of_visits
conversion_rate = np.round(conversion_rate, 5)


# Rounding the values for the dependent variables
frequency_of_visits = np.round(frequency_of_visits, 2)
frequency_of_made_purchase = np.round(frequency_of_made_purchase, 2)


# Creating activity DataFrame
activity_data = pd.DataFrame({
    "Participant ID": demographic_participant_df["Participant ID"],
    "Frequency of Visits": frequency_of_visits,
    "Frequency of Made Purchase": frequency_of_made_purchase,
    "Conversion Rate": conversion_rate})

# Merging the demographic and activity data together
df_factorial = demographic_participant_df.merge(activity_data, on="Participant ID")
# Show the demographic data for the selected participants
df_factorial.head(10)

"""**Violin Plot to Display the Distribution after the Blocking & Randomization for Factorial Design**"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")

# Creating a 'Treatment' column by combining 'Holiday' and 'Discount'
df_factorial['Treatment'] = df_factorial['Holiday'] + " + " + df_factorial['Discount']

# Plotting the violin plot for Conversion Rate
plt.figure(figsize=(10, 6))
sns.violinplot(x="Treatment", y="Conversion Rate", data=df_factorial, hue="Treatment",
               order=["Yes + Percentage Markdown", "Yes + No Discount", "No + Percentage Markdown", "No + No Discount"], legend=False)

# Adding the titles and labels to the plot
plt.title('Factorial Design Effect on Conversion Rate', fontsize=16)
plt.xlabel('Group (Holiday + Discount)', fontsize=12)
plt.ylabel('Conversion Rate', fontsize=12)

# Displaying the plot
plt.show()

""">**Checking for Normality of Dependent Variable: Conversion Rate**"""

from scipy import stats

# Perform Shapiro-Wilk test on Conversion_Rate and print result
shapiroUnem = stats.shapiro(df_factorial['Conversion Rate'].dropna())

print(f'W: {round(shapiroUnem[0],3)}, P-value: {round(shapiroUnem[1],3)}')

# Create a histogram of the Average Order Value
plt.figure(figsize=(10, 6))  # Sets the figure size
plt.hist(df_factorial['Conversion Rate'].dropna(), bins=30, color='blue', alpha=0.7)
plt.title('Distribution of Conversion Rate')  # Adds a title to the histogram
plt.xlabel('Conversion Rate')  # Label for the x-axis
plt.ylabel('Frequency')  # Label for the y-axis
plt.grid(True)  # Adds a grid for better readability
plt.show()  # Displays the plot

""">**TWO-WAY ANOVA & SUMMARY STATISTICS: After blocking using City Type and then randomly assigning each participant into two groups we use two-way ANOVA to find the significance of the blocking factor based on the p-value**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Define the formula for the model
formula = 'Q("Conversion Rate") ~ Q("Group") + Q("Holiday") + Q("Discount") + Q("City_type")+ \
          Q("City_type"):C(Q("Holiday")) + Q("Holiday"):C(Q("Discount")) + \
          C(Q("City_type")):C(Q("Discount"))'

# Fit the model using ordinary least squares (OLS)
model_factorial = ols(formula, data=df_factorial).fit()

# Perform the ANOVA and print the results
anova_table = sm.stats.anova_lm(model_factorial, typ=2)
print(anova_table)
print(model_factorial.summary())

"""**Checking Assumptions for Two-Way ANOVA:**
1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

residuals = model_factorial.resid
# Q-Q Plot to check for normality & heteroscedasticity
sm.qqplot(residuals, line='s')
plt.title('Q-Q Plot of Residuals')
plt.show()

"""**Post Hoc Analysis with Turkey HSD**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd


# Perform the Post-Hoc analysis using Tukey's HSD test
tukey = pairwise_tukeyhsd(df_factorial['Conversion Rate'], df_factorial['Group'], alpha=0.05)

# Print the Tukey HSD results
print("\nTukey HSD Test Results:")
print(tukey)

"""**Section 4.3: CONTINUOUS COVARIATES, ANCOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Adding 1 Continuous Covariate. Using Age as the continuous covariate** <br>
"""

# Setting the seed for reproducibility: so that our analysis and conclusions consistent
np.random.seed(42)

# Select 212 unique random participants
unique_participants = df['Participant ID'].unique()
selected_participants = np.random.choice(unique_participants, 212, replace=False)

# Filter the DataFrame to keep only the selected participants
factorial_df = df[df['Participant ID'].isin(selected_participants)].copy()

# Now only select the demographic features associated with each participant
# Selecting the demographic features associated with each participant
demographic_features = ['Participant ID', 'Age', 'Location']
# Filtering the DataFrame to include only the selected demographic features
# Also, making sure each participant is unique and only appears once
demographic_participant_df = factorial_df[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Now we must randomly assign 53 participants to each of the four groups:
# Group1: Holiday + Percentage Markdown
# Group 2: Holiday + No Discount
# Group 3: No Holiday + Percentage Markdown
# Group 4: No Holiday + No Discount
# Defining the group combinations based on above factorial design
factorial_conditions = [
    # Group 1: Holiday + Percentage Markdown
    ('Yes', 'Percentage Markdown'),
    # Group 2: Holiday + No Discount
    ('Yes', 'No Discount'),
    # Group 3: No Holiday + Percentage Markdown
    ('No', 'Percentage Markdown'),
    # Group 4: No Holiday + No Discount
    ('No', 'No Discount')]

# There will be 4 groups
num_groups = len(factorial_conditions)
# Based on the power analysis we will evenly split 53 individuals per group
group_size = len(demographic_participant_df) // num_groups

# Initializing two lists to store assigned factor levels
holiday_assignments = []
discount_assignments = []

# Randomly assign participants to groups by shuffling the IDs randomly
np.random.shuffle(demographic_participant_df['Participant ID'].values)

# Assigning participants evenly to factorial groups using a loop
for i in range(len(demographic_participant_df)):
  # Cycling through the 4 groups in the given order
  group_index = i % num_groups
  # Getting the corresponding factor levels
  holiday, discount = factorial_conditions[group_index]
  # Appending the assigned holiday condition
  holiday_assignments.append(holiday)
  # Appending the assigned discount condition
  discount_assignments.append(discount)

# Adding the factor assignments to the DataFrame
demographic_participant_df['Holiday'] = holiday_assignments
demographic_participant_df['Discount'] = discount_assignments

# Now we will generate the dependent variables for this experiment specifically
n_samples = len(demographic_participant_df)

# Initializing activity data and using zeros as placeholders
frequency_of_visits = np.zeros(n_samples)
frequency_of_made_purchase = np.zeros(n_samples)

# Looping through each of the rows
for i in range(n_samples):
  # holds the holiday status ('Yes' or 'No') for the current participant
  holiday = holiday_assignments[i]
  # holds the discount type ('Percentage Markdown' or 'No Discount') for the current participant
  discount = discount_assignments[i]

  # For the following condition (i.e. the holiday variable is set to yes)
  if holiday == 'Yes':
    # Then we have to anticipate an increase in the frequency of visits from participants
    # So we create our dependent variables accordingly
    frequency_of_visits[i] = np.random.randint(10, 30)
  else:
    # If there are no holidays the traffic to the platform should be less
    # so we reflect that in the two dependent variables
    frequency_of_visits[i] = np.random.randint(5, 20)

  # For the following condition (i.e. Percentage Markdown is applied)
  # There will be Higher purchases and order value when discounts are applied
  if discount == 'Percentage Markdown':
    # Making sure that the purchases don't exceed the number of visits to solve the
    # case when there are zero visits there should be 0 purchases
    frequency_of_made_purchase[i] = np.random.randint(0, frequency_of_visits[i])
  else:
    # Making sure that the purchases don't exceed the number of visits
    frequency_of_made_purchase[i] = np.random.randint(0, frequency_of_visits[i])


# Calculating conversion rate: which is a just the purchases made to visits
conversion_rate = frequency_of_made_purchase / frequency_of_visits
conversion_rate = np.round(conversion_rate, 5)


# Rounding the values for the dependent variables
frequency_of_visits = np.round(frequency_of_visits, 2)
frequency_of_made_purchase = np.round(frequency_of_made_purchase, 2)


# Creating activity DataFrame
activity_data = pd.DataFrame({
    "Participant ID": demographic_participant_df["Participant ID"],
    "Frequency of Visits": frequency_of_visits,
    "Frequency of Made Purchase": frequency_of_made_purchase,
    "Conversion Rate": conversion_rate})

# Merging the demographic and activity data together
df_factorial = demographic_participant_df.merge(activity_data, on="Participant ID")
# Show the demographic data for the selected participants
df_factorial.head(10)

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Age'is the continuous covariates
Group, Holiday and Discount will be the categorical variable <br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

continuous_covariate = ['Age']
# Define the formula for the model
formula = 'Q("Conversion Rate") ~ Q("Age") + C(Q("Holiday")) + C(Q("Discount")) + \
          Q("Age"):C(Q("Holiday")) + Q("Age"):C(Q("Discount")) + \
          C(Q("Holiday")):C(Q("Discount"))'

# Fit the model using ordinary least squares (OLS)
model_factorial_c = ols(formula, data=df_factorial).fit()

# Perform the ANCOVA and print the results
# Use typ=3 for Type III SS: used in ANCOVA because it tests the main effects and interactions, adjusting for the presence of other factors
anova_table = sm.stats.anova_lm(model_factorial_c, typ=3)
print(anova_table)
print(model_factorial_c.summary())

"""**Checking Assumptions for ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

# Get residuals after fitting the model
residual_factorial_c = model_factorial_c.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_factorial_c, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_factorial_c, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_resid = stats.shapiro(residual_factorial_c.dropna())

print(f'W: {round(shapiro_resid[0], 3)}, P-value: {round(shapiro_resid[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_factorial_c = model_factorial_c.resid
fitted_value = model_factorial_c.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_factorial_c, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Post Hoc Analysis with Turkey HSD**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd


# Perform the Post-Hoc analysis using Tukey's HSD test
tukey1 = pairwise_tukeyhsd(df_factorial['Conversion Rate'], df_factorial['Holiday'], alpha=0.05)
tukey2 = pairwise_tukeyhsd(df_factorial['Conversion Rate'], df_factorial['Discount'], alpha=0.05)

# Print the Tukey HSD results
print("\nTukey HSD Test Results for Holiday:")
print(tukey1)
print("\nTukey HSD Test Results for Discount:")
print(tukey2)

"""**Section 4.4: COMBINING BLOCKING & CONTINUOUS COVARIATES, ANCOVA FOR BOTH, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - BONUS PART: Running another set of two-way ANOVAs with both the blocking factor and continuous covariates for factorial design subset of data.** <br>
"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING City Type -----------------------------------------------------------------------#
# add blocking factor for Factorial design
# Setting the seed for reproducibility: so that our analysis and conclusions consistent
np.random.seed(42)

# Create a new column for city population level
df['City_type'] = ''

# Loop through each row to categorize the city Population
for index, row in df.iterrows():
    if row['City Population'] >= 500000:
        df.at[index, 'City_type'] = 'Big City'
    elif row['City Population'] < 500000:
        df.at[index, 'City_type'] = 'Small City'

# Create a dictionary to store the participants by city population level
city_samples = {}
# Create labels to categorize each of the city popualtion per participant
city_label = ['Big City', 'Small City']

# Looping through each city popualtion level and randomly select 106 participants
for City_type in city_label:
    selected_city = df[df['City_type'] == City_type].sample(n=106, random_state=42)
    city_samples[City_type] = selected_city

# Combine all selected participants from the dictionary into one DataFrame
selected_participants = pd.concat(city_samples.values())

# Find remaining participants who were not selected
remaining_participants = df[~df['Participant ID'].isin(selected_participants['Participant ID'])]

# Count how many participants are in each level
city_level_counts = selected_participants['City_type'].value_counts()
#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING City Type END-----------------------------------------------------------------------#

# Filter the DataFrame to keep only the selected participants
factorial_df = df[df['Participant ID'].isin(selected_participants)].copy()

# Separate participants by City_type
big_city_participants = selected_participants[selected_participants['City_type'] == 'Big City']
small_city_participants = selected_participants[selected_participants['City_type'] == 'Small City']

# We will randomly assign participants from both groups to the 4 groups evenly.
# Randomly assign participants from Big City to 4 groups
big_city_participants['Group'] = np.random.choice(['Group 1', 'Group 2', 'Group 3', 'Group 4'],
                                                   size=big_city_participants.shape[0], replace=True)

# Randomly assign participants from Small City to 4 groups
small_city_participants['Group'] = np.random.choice(['Group 1', 'Group 2', 'Group 3', 'Group 4'],
                                                     size=small_city_participants.shape[0], replace=True)

# Combine the Big City and Small City participants back together
final_selected_participants = pd.concat([big_city_participants, small_city_participants])

# Now we create the demographic and factorial data for these participants
# Filter the original df for demographic features of selected participants
# Add age as a continuous Covariate
demographic_features = ['Participant ID', 'Location','Group', 'City_type', 'Age']
demographic_participant_df = final_selected_participants[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Now we must randomly assign 53 participants to each of the four groups:
# Group1: Holiday + Percentage Markdown
# Group 2: Holiday + No Discount
# Group 3: No Holiday + Percentage Markdown
# Group 4: No Holiday + No Discount
# Defining the group combinations based on above factorial design
factorial_conditions = [
    # Group 1: Holiday + Percentage Markdown
    ('Yes', 'Percentage Markdown'),
    # Group 2: Holiday + No Discount
    ('Yes', 'No Discount'),
    # Group 3: No Holiday + Percentage Markdown
    ('No', 'Percentage Markdown'),
    # Group 4: No Holiday + No Discount
    ('No', 'No Discount')]

# There will be 4 groups
num_groups = len(factorial_conditions)
# Based on the power analysis we will evenly split 53 individuals per group
group_size = len(demographic_participant_df) // num_groups

# Initializing two lists to store assigned factor levels
holiday_assignments = []
discount_assignments = []

# Assigning d groups of participants to factorial groups using a loop
for index, row in demographic_participant_df.iterrows():
    # Get the group assigned to the participant
    group = row['Group']
    # Map the group to the corresponding factorial condition (Holiday, Discount)
    if group == 'Group 1':
        holiday, discount = factorial_conditions[0]
    elif group == 'Group 2':
        holiday, discount = factorial_conditions[1]
    elif group == 'Group 3':
        holiday, discount = factorial_conditions[2]
    elif group == 'Group 4':
        holiday, discount = factorial_conditions[3]
    # Append the corresponding holiday and discount values
    holiday_assignments.append(holiday)
    discount_assignments.append(discount)

# Adding the factor assignments to the DataFrame
demographic_participant_df['Holiday'] = holiday_assignments
demographic_participant_df['Discount'] = discount_assignments

# Now we will generate the dependent variables for this experiment specifically
n_samples = len(demographic_participant_df)

# Initializing activity data and using zeros as placeholders
frequency_of_visits = np.zeros(n_samples)
frequency_of_made_purchase = np.zeros(n_samples)

# Looping through each of the rows
for i in range(n_samples):
  # holds the holiday status ('Yes' or 'No') for the current participant
  holiday = holiday_assignments[i]
  # holds the discount type ('Percentage Markdown' or 'No Discount') for the current participant
  discount = discount_assignments[i]

  # For the following condition (i.e. the holiday variable is set to yes)
  if holiday == 'Yes':
    # Then we have to anticipate an increase in the frequency of visits from participants
    # So we create our dependent variables accordingly
    frequency_of_visits[i] = np.random.randint(10, 30)
  else:
    # If there are no holidays the traffic to the platform should be less
    # so we reflect that in the two dependent variables
    frequency_of_visits[i] = np.random.randint(5, 20)

  # For the following condition (i.e. Percentage Markdown is applied)
  # There will be Higher purchases and order value when discounts are applied
  if discount == 'Percentage Markdown':
    # Making sure that the purchases don't exceed the number of visits to solve the
    # case when there are zero visits there should be 0 purchases
    frequency_of_made_purchase[i] = np.random.randint(0, frequency_of_visits[i])
  else:
    # Making sure that the purchases don't exceed the number of visits
    frequency_of_made_purchase[i] = np.random.randint(0, frequency_of_visits[i])


# Calculating conversion rate: which is a just the purchases made to visits
conversion_rate = frequency_of_made_purchase / frequency_of_visits
conversion_rate = np.round(conversion_rate, 5)


# Rounding the values for the dependent variables
frequency_of_visits = np.round(frequency_of_visits, 2)
frequency_of_made_purchase = np.round(frequency_of_made_purchase, 2)


# Creating activity DataFrame
activity_data = pd.DataFrame({
    "Participant ID": demographic_participant_df["Participant ID"],
    "Frequency of Visits": frequency_of_visits,
    "Frequency of Made Purchase": frequency_of_made_purchase,
    "Conversion Rate": conversion_rate})

# Merging the demographic and activity data together
df_factorial = demographic_participant_df.merge(activity_data, on="Participant ID")

# Show the demographic data for the selected participants
df_factorial.head(10)

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Age' is the continuous covariates, City Type will be a categorical blocking factor, and Group, Holiday and Discount will be the categorical variable<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Create model
formula = 'Q("Conversion Rate") ~ Q("Age") + C(Q("City_type")) + C(Q("Group")) + C(Q("Holiday")) + C(Q("Discount"))'

# Fit the model using ordinary least squares (OLS)
model_factorial_2 = sm.formula.ols(formula, data=df_factorial).fit()

# Perform ANOVA and print the table
anova_table = sm.stats.anova_lm(model_factorial_2, typ=2)  # Using Type II SS
print(anova_table)

# Print a detailed summary of the model
print(model_factorial_2.summary())

"""**Checking Assumptions for COMBINED ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_factorial_2 = model_factorial_2.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_factorial_2, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_factorial_2, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_residual = stats.shapiro(residual_factorial_2.dropna())

print(f'W: {round(shapiro_residual[0], 3)}, P-value: {round(shapiro_residual[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_factorial_2 = model_factorial_2.resid
fitted_value = model_factorial_2.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_factorial_2, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Post Hoc Analysis with Turkey HSD**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd


# Perform the Post-Hoc analysis using Tukey's HSD test
tukey1 = pairwise_tukeyhsd(df_factorial['Conversion Rate'], df_factorial['Group'], alpha=0.05)

# Print the Tukey HSD results
print("\nTukey HSD Test Results:")
print(tukey1)

"""**Flowchart for Factorial design with the blocking factor**"""

!pip install schemdraw
import schemdraw
from schemdraw import flow

with schemdraw.Drawing() as d:

    d.config(fontsize=11)
    A = flow.Box(w=5, h=5).label('H0: The mean conversion \n rates between \n Group A (T1T2), \n Group B (T1C2), \n Group C (T2C1), \n and Group D (C1C2) \n are all equal').at((0, 0))
    A1 = flow.Box(w=5, h=5).label('HA: The mean conversion \n rates between \n Group A (T1T2), \n Group B (T1C2), \n Group C (T2C1), \n and Group D (C1C2) \n are all not equal').at((0, -7))
    b = flow.Start().label('Factorial Design').at((8.5,-3))
    flow.Arc2(k=.1, arrow='->').at(A.E).to(b.N)
    flow.Arc2(k=-.1, arrow='->').at(A1.E).to(b.S)
    flow.Arrow().right(d.unit/2).at(b.E)
    d1 = flow.Box(w=7, h=3.9).label('Blocking factor: City type \n Big City \n Small City')
    flow.Arrow().right(d.unit* 1).at(d1.E)
    d1 = flow.Box(w=5, h=5).label('Randomization: \n We randomly select \n 212 unique participants \n from synthetic data \n for the experiment. \n The features we keep \n in this experiment are the \n Demographical info, and \n later add some of \n the dependent variables \n in order to \n measure the results \n for conversion rate.')
    flow.Arrow().up(d.unit*1).at(d1.N)
    d2 = flow.Decision(w=7, h=3.9).label('Group 1')
    flow.Arrow().down(d.unit*1).at(d1.S)
    d3 = flow.Decision(w=7, h=3.9).label('Group 2')
    flow.Arrow().right(d.unit*2).at(d1.S)
    d4 = flow.Decision(w=7, h=3.9).label('Group 3')
    flow.Arrow().right(d.unit*2).at(d1.N)
    d5 = flow.Decision(w=7, h=3.9).label('Group 4')

    flow.Arrow().right(d.unit*1.5).at(d2.E)
    d6 = flow.Box(w=5.3, h=4.0).anchor('W').label('Treatment1 + Treatment2: \n Holiday + Percentage Markdown')

    flow.Arrow().right(d.unit*1.5).at(d3.E)
    d7 = flow.Box(w=5.3, h=4.0).anchor('W').label('Treatment1 + Control2: \n Holiday + No Discount')

    flow.Arrow().right(d.unit*1.5).at(d4.E)
    d8 = flow.Box(w=6, h=4.0).anchor('W').label('Control1 + Treatment2: \n No Holiday + Percentage Markdown')

    flow.Arrow().right(d.unit*1.5).at(d5.E)
    d9 = flow.Box(w=5.3, h=4.0).anchor('W').label('Control1 + Control2: \n No Holiday + No Discount')


    flow.Arrow().right(d.unit*4.5).at(d6.E)
    C = flow.Box(w=6, h=2).label('Analyze Effects on Conversion Rate \n based on ANOVA')
    flow.Arc2(k=-.6, arrow='->').at(d7.E).to(C.SE)
    flow.Arc2(k=-.1, arrow='->').at(d8.E).to(C.S)
    flow.Arc2(k=-.1, arrow='->').at(d9.E).to(C.SW)

    flow.Arrow().right(d.unit*2.5).at(C.E)
    D = flow.Box(w=6, h=2).label('Accept OR Reject Null Hypothesis \n using Decision Rule')

"""# --------------------- END OF FACTORIAL DESIGN CODE ----------------------------

#**5. Matched Pairs Design**

**Section 5.1: Power Analysis, Sample Size, & Associated Graph** </br>
Power: 0.85 <br>
Effect Size: Medium (0.5)</br>
Sample Size: 76</br>
Dependent Variable: Session Duration<br>
"""

from statsmodels.stats.power import FTestAnovaPower

# Power Analysis for Matched Pair Design Scenario with Three Conditions

# Medium effect size
effect_size = 0.5

# Significance level (alpha)
alpha = 0.05

# Desired power
power = 0.85

# Creating a power analysis object for F-test (ANOVA)
analysis = FTestAnovaPower()

# Calculating the required sample size per group using the effect size
required_sample_size_per_group = analysis.solve_power(effect_size=effect_size,k_groups=number_groups, alpha=alpha, power=power)

# Round the result to the nearest integer
required_sample_size_per_group = round(required_sample_size_per_group)

# Total sample size for the experiment (across all three groups)
total_sample_size = required_sample_size_per_group * number_groups

# Output the results
print(f"Required sample size per group: {required_sample_size_per_group}")
print(f"Total sample size for all groups: {total_sample_size}")

from numpy import array
from matplotlib import pyplot

# Power vs. number of observations
# Parameters for power analysis
# The larger the effect size, the less likely it is to be random error.
effect_sizes = array([0.2, 0.5, 0.8])
sample_sizes = array(range(2, 76))


# Calculate power curves from multiple power analyses.
# Assume a significance of 0.05 and explore the change in sample size between 2 and 100 with low (es=0.2), medium (es=0.5), and high effect (es=0.8)sizes.
analysis = FTestAnovaPower()
analysis.plot_power(dep_var='nobs', nobs=sample_sizes, effect_size=effect_sizes, alpha=0.05, title='Power of ANOVA Test')
pyplot.grid()
pyplot.ylabel('Power')
pyplot.show()

"""**Section 5.2: BLOCKING, TWO-WAY ANOVA, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - Blocking for Matched Pair Design using Age Group as blocking factor** <br>

> People who have previously exposed to social media promotions might stay shorter online to browse and purchase, because they might have alread known what they want and what to buy from the website. regardless of the promotion. <br>
Confounding Effect: If a specific promotion is on people who have exposed to social media promotion VS people who have not exposed to social media promotion, the promotion's effect on session duration may be overestimated. <br>

"""

# Setting the seed for reproducibility
np.random.seed(42)

# Select 38 people who have been exposed to social media promotion and 38 participants who have not been exposed
exposed_participants = df[df['Exposure to Social Media Promotion'] == 'Yes']['Participant ID'].unique()
non_exposed_participants = df[df['Exposure to Social Media Promotion'] == 'No']['Participant ID'].unique()

selected_exposed = np.random.choice(exposed_participants, 38, replace=False)
selected_non_exposed = np.random.choice(non_exposed_participants, 38, replace=False)

# Combine selected participants
selected_participants = np.concatenate([selected_exposed, selected_non_exposed])

# Filter the DataFrame to keep only the selected participants
matched_pairs_df = df[df['Participant ID'].isin(selected_participants)].copy()

""">**Randomization for Matched Pair Design After Blocking Done Above**"""

# Setting the seed for reproducibility
np.random.seed(42)
# Selecting demographic features
demographic_features = ['Participant ID', 'Age', 'Gender', 'Location', 'Income', 'Exposure to Social Media Promotion']
demographic_participant_df = matched_pairs_df[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Sorting for better pair matching based on 'Exposure to Social Media Promotion' and 'Age'
demographic_participant_df.sort_values(by=['Exposure to Social Media Promotion', 'Age'], inplace=True)

# Reset the pairing track and dictionary
paired_participants = {}
paired_participant_track = set()
pair_count = 1

# Defining the treatments
treatments = ['Percentage Markdown']
control = 'No Discount'

# Convert dataframe to list for easier iteration
participant_list = demographic_participant_df.values.tolist()

# Creating a dictionary to store the assigned treatment for each participant
participant_treatment_mapping = {}

# Iterating through participants to form pairs based on same exposure to social media promotion
for i in range(len(participant_list)):
    if participant_list[i][0] in paired_participant_track:
        continue

    for j in range(i + 1, len(participant_list)):
        if participant_list[j][0] in paired_participant_track:
            continue

        # Checking if the participants have the same exposure status
        if participant_list[i][5] == participant_list[j][5]:  # 'Exposure to Social Media Promotion' is indexed at 5
            paired_participant_track.add(participant_list[i][0])
            paired_participant_track.add(participant_list[j][0])

            # Randomly decide which participant gets the treatment
            if np.random.rand() < 0.5:
                treatment_receiver = participant_list[i][0]
                control_receiver = participant_list[j][0]
            else:
                treatment_receiver = participant_list[j][0]
                control_receiver = participant_list[i][0]

            # Randomly select a treatment
            assigned_treatment = np.random.choice(treatments)

            # Assigning treatment and control
            participant_treatment_mapping[treatment_receiver] = assigned_treatment
            participant_treatment_mapping[control_receiver] = control

            # Storing the pair in the dictionary
            paired_participants[pair_count] = {
                'Pair': {'Participant 1': participant_list[i], 'Participant 2': participant_list[j]},
                'Treatment Receiver': treatment_receiver, 'Assigned Treatment': assigned_treatment,
                'Control Receiver': control_receiver, 'Control Group': control
            }

            pair_count += 1
            break  # Move to the next unpaired participant

# Converting the treatment mapping back into a DataFrame
treatment_df = pd.DataFrame(list(participant_treatment_mapping.items()), columns=['Participant ID', 'Assigned Condition'])

# Merging the treatment information with the demographic DataFrame
paired_participant_df = demographic_participant_df.merge(treatment_df, on='Participant ID', how='left')

# Defining session duration based on treatment assignment
session_durations = {"Percentage Markdown": (700, 2500), "No Discount": (0, 1500)}

# Defining a function to create a column for our dependent variable: Session duration
def generate_session_duration(condition):
    if condition in session_durations:
        min_duration, max_duration = session_durations[condition]
        return round(np.random.uniform(min_duration, max_duration), 2)
    return 0

# Applying the function to generate session durations
paired_participant_df['Session Duration'] = paired_participant_df['Assigned Condition'].apply(generate_session_duration)

print(f"Total number of pairs created: {len(paired_participants)}")
print('\n')

# Preview the updated dataframe with treatment assignments
paired_participant_df.head(76)

"""**List out all combinations after adding blocking factor**"""

# Add a column to identify the treatment/control status for each participant
paired_participant_df['Treatment Status'] = paired_participant_df['Assigned Condition'].apply(lambda x: 'Treatment' if x == 'Percentage Markdown' else 'Control')

# Create combinations of Exposure to Social Media Promotion, Gender, and Treatment Status
combinations_count = paired_participant_df.groupby(['Exposure to Social Media Promotion', 'Gender', 'Treatment Status']).size().reset_index(name='Count')

# Display the count of each combination
print(combinations_count)

""">**Checking for Normality of Dependent Variable: Session Duration**"""

# Create a histogram of the Conversion Rate
plt.figure(figsize=(10, 6))
plt.hist(paired_participant_df['Session Duration'].dropna(), bins=30, color='blue', alpha=0.7)
# Adds a title to the histogram
plt.title('Distribution of Session Duration (i.e. Ratios)')
# Label for the x-axis
plt.xlabel('Session Duration')
# Label for the y-axis
plt.ylabel('Frequency')
# Adds a grid for better readability
plt.grid(True)
# Displays the plot
plt.show()

from scipy import stats

# Perform Shapiro-Wilk test on Conversion_Rate and print result
shapiroUnem = stats.shapiro(paired_participant_df['Session Duration'].dropna())

print(f'W: {round(shapiroUnem[0],3)}, P-value: {round(shapiroUnem[1],3)}')

""">**TWO-WAY ANOVA & SUMMARY STATISTICS: After blocking using age group and then randomly assigning each participant into pairs we use two-way ANOVA to find the significance of the blocking factor based on the p-value**"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Define the formula for the model
# Here 'Session Duration' is set as the dependent variable
# 'Exposure to Social Media Promotion', 'Assigned Condition', and 'Gender' are the independent variables
formula = 'Q("Session Duration") ~ Q("Exposure to Social Media Promotion") + Q("Assigned Condition") + Q("Gender") + Q("Exposure to Social Media Promotion"):Q("Assigned Condition") + Q("Exposure to Social Media Promotion"):Q("Gender") + Q("Assigned Condition"):Q("Gender") + Q("Exposure to Social Media Promotion"):Q("Assigned Condition"):Q("Gender")'

# Fit the model using ordinary least squares (OLS)
model_matched = ols(formula, data=paired_participant_df).fit()

# Perform the ANOVA and print the results
anova_table = sm.stats.anova_lm(model_matched, typ=2) # typ=2 for ANOVA Type II which considers each factor's effect adjusted for other factors
print(anova_table)
print(model_matched.summary())

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_matched_pair = pairwise_tukeyhsd(
    paired_participant_df['Session Duration'],
    paired_participant_df['Assigned Condition']
)

print("\nTukey HSD Test Results:")
print(tukey_results_matched_pair)

"""**Checking Assumptions for Two-Way ANOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residuals_matched = model_matched.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residuals_matched, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residuals_matched, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

""">**Performing Shapiro-Wilk test on residuals after fitting the model to check for assumption of normality.**

> The residuals of our model seem to follow a normal distribution, as the Shapiro-Wilk test results indicate there is no significant deviation from normality. It implies that the assumptions of normality for our residuals are likely met, which is an important condition for fitting our model.
"""

# Perform Shapiro-Wilk test on residuals
shapiro_res = stats.shapiro(residuals_matched.dropna())

print(f'W: {round(shapiro_res[0], 3)}, P-value: {round(shapiro_res[1], 3)}')

""">**The Residuals vs. Fitted Values Plot:** below indicates that the model is a good fit for the data, as there are no clear patterns, trends, or heteroscedasticity visible in the residuals."""

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residuals_matched = model_matched.resid
fitted_values = model_matched.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_values, y=residuals_matched, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Section 5.3: CONTINUOUS COVARIATES, ANCOVA, & ASSUMPTION CHECKS** <br>

>**Assignment 2 - Adding 1 Continuous Covariate (Income) ** <br>
"""

# Setting the seed for reproducibility
np.random.seed(42)

# Select 76 unique random participants
unique_participants = df['Participant ID'].unique()
selected_participants = np.random.choice(unique_participants, 76, replace=False)

# Filter the DataFrame to keep only the selected participants
matched_pairs_df = df[df['Participant ID'].isin(selected_participants)].copy()
#--------------------------------------------------------------------- Assignment 2: Adding a New Covariate --------------------------------------------------------------------------#
# Selecting demographic features associated with each participant
demographic_features = ['Participant ID', 'Age', 'Gender', 'Location', 'Income']
demographic_participant_df = matched_pairs_df[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Sorting participants by Gender and then by Age for easier pairing
demographic_participant_df.sort_values(by=['Gender', 'Age'], inplace=True)

# Initialize dictionary to store participant pairs and track paired participants
paired_participants = {}
paired_participant_track = set()
pair_count = 1
treatments = ['Percentage Markdown']
control = 'No Discount'

# Convert dataframe to list for easier iteration
participant_list = demographic_participant_df.values.tolist()

# Creating a dictionary to store the assigned treatment for each participant
participant_treatment_mapping = {}

# Iterating through participants to form pairs
for i in range(len(participant_list)):
    if participant_list[i][0] in paired_participant_track:
        continue

    for j in range(i + 1, len(participant_list)):
        if participant_list[j][0] in paired_participant_track:
            continue

        # Pair participants only based on same gender
        if participant_list[i][2] == participant_list[j][2]:  # participant_list[i][2] is Gender
            paired_participant_track.add(participant_list[i][0])
            paired_participant_track.add(participant_list[j][0])

            # Randomly assign treatment or control
            if np.random.rand() < 0.5:
                treatment_receiver = participant_list[i][0]
                control_receiver = participant_list[j][0]
            else:
                treatment_receiver = participant_list[j][0]
                control_receiver = participant_list[i][0]

            assigned_treatment = np.random.choice(treatments)

            # Assign treatment and control
            participant_treatment_mapping[treatment_receiver] = assigned_treatment
            participant_treatment_mapping[control_receiver] = control

            # Store the pair information
            paired_participants[pair_count] = {
                'Pair': {'Participant 1': participant_list[i], 'Participant 2': participant_list[j]},
                'Treatment Receiver': treatment_receiver, 'Assigned Treatment': assigned_treatment,
                'Control Receiver': control_receiver, 'Control Group': control
            }

            pair_count += 1
            break  # Move to next unpaired participant

# Convert the treatment mapping into a DataFrame
treatment_df = pd.DataFrame(list(participant_treatment_mapping.items()), columns=['Participant ID', 'Assigned Condition'])

# Merge the treatment information with the demographic DataFrame
paired_participant_df = demographic_participant_df.merge(treatment_df, on='Participant ID', how='left')

# Define criteria for session duration based on treatment assignment
session_durations = {"Percentage Markdown": (700, 2500), "No Discount": (0, 1500)}

# Function to generate session durations
def generate_session_duration(condition):
    if condition in session_durations:
        min_duration, max_duration = session_durations[condition]
        return round(np.random.uniform(min_duration, max_duration), 2)
    return 0

# Apply the function to generate session durations
paired_participant_df['Session Duration'] = paired_participant_df['Assigned Condition'].apply(generate_session_duration)

# Preview the updated dataframe with treatment assignments
paired_participant_df.head(10)

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Income' continuous covariates.
Gender and Assigned Condition will be the categorical variables (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Assuming 'paired_participant_df' is the DataFrame that contains all necessary data
# Update the formula to focus on the independent variables and their interactions, now including 'Gender'
formula = 'Q("Session Duration") ~ Q("Income") + Q("Assigned Condition") + Q("Gender") + \
          Q("Income"):Q("Assigned Condition") + Q("Assigned Condition"):Q("Gender") + \
          Q("Income"):Q("Gender")'

# Fit the model using ordinary least squares (OLS)
model_matched_c = ols(formula, data=paired_participant_df).fit()

# Perform the ANOVA and print the results
anova_table = sm.stats.anova_lm(model_matched_c, typ=3)  # Type II sums of squares are used here
print(anova_table)

# Print a detailed summary of the model to see coefficients, standard errors, p-values, and more
print(model_matched_c.summary())

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_matched_pair = pairwise_tukeyhsd(
    paired_participant_df['Session Duration'],
    paired_participant_df['Assigned Condition']
)

print("\nTukey HSD Test Results:")
print(tukey_results_matched_pair)

"""**Checking Assumptions for ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_matched_c = model_matched_c.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_matched_c, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_matched_c, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_resid = stats.shapiro(residual_matched_c.dropna())

print(f'W: {round(shapiro_resid[0], 3)}, P-value: {round(shapiro_resid[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_matched_c = model_matched_c.resid
fitted_value = model_matched_c.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_matched_c, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**Section 5.4: COMBINING BLOCKING & CONTINUOUS COVARIATES, ANCOVA FOR BOTH, & ASSUMPTION CHECKS** <br>

><mark>**Assignment 2 - BONUS PART: Running another set of two-way ANOVAs with both the blocking factor and continuous covariates for Matched Pair design subset of data.** <br>
"""

#-----------------------------------------------------------------ASSIGNMENT 2: BLOCKING USING INCOME LEVELS -----------------------------------------------------------------------#
# Setting the seed for reproducibility
np.random.seed(42)

# Select 38 people who have been exposed to social media promotion and 38 participants who have not been exposed
exposed_participants = df[df['Exposure to Social Media Promotion'] == 'Yes']['Participant ID'].unique()
non_exposed_participants = df[df['Exposure to Social Media Promotion'] == 'No']['Participant ID'].unique()

selected_exposed = np.random.choice(exposed_participants, 38, replace=False)
selected_non_exposed = np.random.choice(non_exposed_participants, 38, replace=False)

# Combine selected participants
selected_participants = np.concatenate([selected_exposed, selected_non_exposed])

# Filter the DataFrame to keep only the selected participants
matched_pairs_df = df[df['Participant ID'].isin(selected_participants)].copy()

#------------------------------------------------------------------- Assignment 2: Adding a New Covariate --------------------------------------------------------------------------#
# Selecting demographic features
demographic_features = ['Participant ID', 'Age', 'Gender', 'Location', 'Income', 'Exposure to Social Media Promotion']
demographic_participant_df = matched_pairs_df[demographic_features].drop_duplicates(subset='Participant ID').copy()

# Sorting for better pair matching based on 'Exposure to Social Media Promotion' and 'Age'
demographic_participant_df.sort_values(by=['Exposure to Social Media Promotion', 'Age'], inplace=True)

# Reset the pairing track and dictionary
paired_participants = {}
paired_participant_track = set()
pair_count = 1

# Defining the treatments
treatments = ['Percentage Markdown']
control = 'No Discount'

# Convert dataframe to list for easier iteration
participant_list = demographic_participant_df.values.tolist()

# Creating a dictionary to store the assigned treatment for each participant
participant_treatment_mapping = {}

# Iterating through participants to form pairs based on same exposure to social media promotion
for i in range(len(participant_list)):
    if participant_list[i][0] in paired_participant_track:
        continue

    for j in range(i + 1, len(participant_list)):
        if participant_list[j][0] in paired_participant_track:
            continue

        # Checking if the participants have the same exposure status
        if participant_list[i][5] == participant_list[j][5]:  # 'Exposure to Social Media Promotion' is indexed at 5
            paired_participant_track.add(participant_list[i][0])
            paired_participant_track.add(participant_list[j][0])

            # Randomly decide which participant gets the treatment
            if np.random.rand() < 0.5:
                treatment_receiver = participant_list[i][0]
                control_receiver = participant_list[j][0]
            else:
                treatment_receiver = participant_list[j][0]
                control_receiver = participant_list[i][0]

            # Randomly select a treatment
            assigned_treatment = np.random.choice(treatments)

            # Assigning treatment and control
            participant_treatment_mapping[treatment_receiver] = assigned_treatment
            participant_treatment_mapping[control_receiver] = control

            # Storing the pair in the dictionary
            paired_participants[pair_count] = {
                'Pair': {'Participant 1': participant_list[i], 'Participant 2': participant_list[j]},
                'Treatment Receiver': treatment_receiver, 'Assigned Treatment': assigned_treatment,
                'Control Receiver': control_receiver, 'Control Group': control
            }

            pair_count += 1
            break  # Move to the next unpaired participant

# Converting the treatment mapping back into a DataFrame
treatment_df = pd.DataFrame(list(participant_treatment_mapping.items()), columns=['Participant ID', 'Assigned Condition'])

# Merging the treatment information with the demographic DataFrame
paired_participant_df = demographic_participant_df.merge(treatment_df, on='Participant ID', how='left')

# Defining session duration based on treatment assignment
session_durations = {"Percentage Markdown": (700, 2500), "No Discount": (0, 1500)}

# Defining a function to create a column for our dependent variable: Session duration
def generate_session_duration(condition):
    if condition in session_durations:
        min_duration, max_duration = session_durations[condition]
        return round(np.random.uniform(min_duration, max_duration), 2)
    return 0

# Applying the function to generate session durations
paired_participant_df['Session Duration'] = paired_participant_df['Assigned Condition'].apply(generate_session_duration)

print(f"Total number of pairs created: {len(paired_participants)}")
print('\n')

# Preview the updated dataframe with treatment assignments
paired_participant_df.head(76)

"""**ANCOVA & SUMMARY STATISTICS:** <br>

> 'Income' is the continuous covariates, Age Group will be a categorical blocking factor, and Gender and Treatment will be the categorical variables (i.e. treatment/control groups)<br>

>**SOURCE FOR ANCOVA CODE:** https://www.reneshbedre.com/blog/ancova.html
"""

#------------------------------------------------ TWO-WAY ANOVA FOR BLOCKING & COVARIATES ------------------------------------------------------------------------------------------#
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Assuming 'df' is your DataFrame and contains the appropriate data
# Create the model formula, including the main effects and interactions
formula = 'Q("Session Duration") ~ Q("Income") + Q("Exposure to Social Media Promotion") + Q("Gender") + Q("Assigned Condition") + \
           Q("Income"):Q("Exposure to Social Media Promotion") + Q("Income"):Q("Gender") + Q("Income"):Q("Assigned Condition") + \
           Q("Exposure to Social Media Promotion"):Q("Gender") + Q("Exposure to Social Media Promotion"):Q("Assigned Condition") + \
           Q("Gender"):Q("Assigned Condition")'

# Fit the model using ordinary least squares (OLS)
model_matched_2 = ols(formula, data=paired_participant_df).fit()

# Perform ANOVA and print table
aov_table = sm.stats.anova_lm(model_matched_2, typ=2)  # Using Type II sums of squares
print(aov_table)

# Print a detailed summary of the model
print(model_matched_2.summary())

#------------------------------------------------ TWO-WAY ANOVA FOR BLOCKING & COVARIATES END------------------------------------------------------------------------------------------#

#------------------------------------------------------------------------------ TUKEY HSD TEST -----------------------------------------------------------------------------------------------#
# Perform Tukey's HSD for DiscountType
tukey_results_matched_pair = pairwise_tukeyhsd(
    paired_participant_df['Session Duration'],
    paired_participant_df['Assigned Condition']
)

print("\nTukey HSD Test Results:")
print(tukey_results_matched_pair)

"""**Checking Assumptions for COMBINED ANCOVA:**

> 1. IID: Independently & Identically distributed taken care of by random sampling.
2. Normality: Testing for normality of residuals after fitting the model using the Shapiro-Wilk test
3. Homoscedasticity: variances of the dependent variable should be approximately equal across the different groups using
"""

import seaborn as sns
import statsmodels.api as sm

# Get residuals after fitting the model
residual_matched_2 = model_matched_2.resid

# Visualization: KDE plot and Q-Q plot for residuals
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

# KDE plot for residuals with fill instead of shade
sns.kdeplot(residual_matched_2, ax=ax[0], color='red', fill=True)
ax[0].set_title('Plot of Residuals')

# Q-Q plot
sm.qqplot(residual_matched_2, line='s', ax=ax[1])
ax[1].set_title('Q-Q Plot of Residuals')

# Plotting the graphs below
plt.tight_layout()
plt.show()

# Perform Shapiro-Wilk test on residuals
shapiro_residual = stats.shapiro(residual_matched_2.dropna())

print(f'W: {round(shapiro_residual[0], 3)}, P-value: {round(shapiro_residual[1], 3)}')

import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# Get the residuals and the fitted values
residual_matched_2 = model_matched_2.resid
fitted_value = model_matched_2.fittedvalues

# Plot Residuals vs. Fitted Values
plt.figure(figsize=(10, 6))
sns.scatterplot(x=fitted_value, y=residual_matched_2, alpha=0.7)
# Line at zero as a reference
plt.axhline(0, color='red', linestyle='--', linewidth=2)
plt.title("Residuals vs. Fitted Values Plot")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.tight_layout()
plt.show()

"""**The Flow Chart for Matched Pairs Design with the blocking factor**"""

!pip install schemdraw
import schemdraw
from schemdraw import flow

with schemdraw.Drawing() as d:

    d.config(fontsize=11)
    b = flow.Start().label('Matched Pairs Design').at((8.5,-3))

    flow.Arrow().right(d.unit/2).at(b.E)
    d1 = flow.Box(w=8, h=5).label('Adding Blocking factor: \nExposure to social media promotion \n Categorize paticipants into \n people who have exposed \n to social media promotion \n before and people who \n have not exposed to social \n media promotion before')
    flow.Arrow().right(d.unit*1).at(d1.E)
    d1 = flow.Box(w=5, h=5).label('Matched Pair process:\n match same gender in a pair \nto receive treatment or control')
    flow.Arrow().right(d.unit*1).at(d1.E)
    d1 = flow.Box(w=8, h=5).label('Randomization:  \n We selected 38 participants \nfrom each group, and then we randomly \n pair same gender participants \n to receive treatment or \n control for the experiment')
    flow.Arrow().right(d.unit*1).at(d1.E)
    d1 = flow.Box(w=5, h=5).label('2 Participants in a pair \n are randomly assigned \n with treatment or control')
    flow.Arrow().up(d.unit*1).at(d1.N)
    d2 = flow.Decision(w=7, h=3.9).label('Participant 1 in a pair')
    flow.Arrow().down(d.unit*1).at(d1.S)
    d3 = flow.Decision(w=7, h=3.9).label('Participant 2 in a pair')



    flow.Arrow().right(d.unit*1.5).at(d2.E)
    d6 = flow.Box(w=5.3, h=4.0).anchor('W').label('Receive Percentage Markdown')

    flow.Arrow().right(d.unit*1.5).at(d3.E)
    d7 = flow.Box(w=5.3, h=4.0).anchor('W').label('Receive No Discount')




    flow.Arrow().right(d.unit*2).at(d6.E)
    C = flow.Box(w=6, h=4).label('There are 38 participants \nin the treatment group \nand 38 participants in \nthe control group ')
    flow.Arc2(k=-.4, arrow='->').at(d7.E).to(C.S)


    flow.Arrow().right(d.unit*2.5).at(C.E)
    D = flow.Box(w=6, h=4).label('Apply Two way ANOVA \nto analyze the difference \nbetween treatment and control \ngroup with the blocking factor')